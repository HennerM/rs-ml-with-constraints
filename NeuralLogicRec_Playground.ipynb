{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process batch\n",
      "Processing batch done\n",
      "Process batch\n",
      "Processing batch done\n",
      "Process batch\n",
      "Processing batch done\n",
      "Process batch\n",
      "Processing batch done\n",
      "Process batch\n",
      "Processing batch done\n",
      "Process batch\n",
      "Processing batch done\n",
      "Process batch\n",
      "Processing batch done\n",
      "Process batch\n",
      "Processing batch done\n",
      "Process batch\n",
      "Processing batch done\n",
      "Process batch\n",
      "Processing batch done\n",
      "Process batch\n",
      "Processing batch done\n",
      "Process batch\n",
      "Processing batch done\n",
      "Process batch\n",
      "Processing batch done\n",
      "Process batch\n",
      "Processing batch done\n",
      "Process batch\n",
      "Processing batch done\n",
      "Process batch\n",
      "Processing batch done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import utils.common\n",
    "import evaluation\n",
    "import importlib\n",
    "import numpy as np\n",
    "import time\n",
    "from models.ConstraintAutoRec import ConstraintAutoRec \n",
    "import models.NeuralLogicRec\n",
    "import tensorflow as tf\n",
    "importlib.reload(utils.common)\n",
    "importlib.reload(evaluation)\n",
    "importlib.reload(models.NeuralLogicRec)\n",
    "\n",
    "ml_small = utils.common.ml_small\n",
    "ml_big = utils.common.movie_lens\n",
    "ev = evaluation.Evaluation(ml_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlr = models.NeuralLogicRec.NLR(ml_big['user'], ml_big['dimensions'], epochs=1, embedding_dim =16, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 Loss at step 6924: 0.0798, time: 2195.534\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    nlr.train(utils.common.load_dataset(ml_big), ml_big['train']['records'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch nr 1 predicted\n",
      "Batch nr 2 predicted\n",
      "Batch nr 3 predicted\n",
      "Batch nr 4 predicted\n",
      "Batch nr 5 predicted\n",
      "Batch nr 6 predicted\n",
      "Batch nr 7 predicted\n",
      "Batch nr 8 predicted\n",
      "Batch nr 9 predicted\n",
      "Batch nr 10 predicted\n",
      "Batch nr 11 predicted\n",
      "Batch nr 12 predicted\n",
      "Batch nr 13 predicted\n",
      "Batch nr 14 predicted\n",
      "Batch nr 15 predicted\n",
      "Batch nr 16 predicted\n",
      "waiting for queue\n",
      "processing results\n",
      "Evaluating NeuralLogicRec |============================================================>| 100.0% \n",
      "{'accuracy': 0.24737946123076643, 'precision': 0.8373605196245728, 'recall': 0.12352258603209806, 'map@1': 0.01318359375, 'map@5': 0.003406575520833334, 'map@10': 0.002296736782820767, 'diversity@5': 0.38533489970631, 'diversity@10': 0.38293860016859355, 'epc@5': 0.9735711610314445, 'epc@10': 0.9783034950377194, 'epd@5': 0.3814487192376029, 'name': 'NeuralLogicRec', 'latent_dim': 128, 'epochs': 1, 'batch_size': 20}\n"
     ]
    }
   ],
   "source": [
    "print(ev.evaluate(nlr, max_nr_batches=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(np.flip(np.sort(nlr.predict_single_user(0))))\n",
    "# print(nlr.predict_single_user(1))\n",
    "# nlr.model(np.array(1), np.array(1))\n",
    "user = 0\n",
    "nr_items = 300\n",
    "items = np.arange(nr_items)\n",
    "user = np.repeat(user, nr_items)\n",
    "predictions = nlr.model(user, items)\n",
    "print(np.sort(predictions['rec']))\n",
    "rec = models.NeuralLogicRec.map_inference(nlr.model, predictions, 1e-7)['rec']\n",
    "print(np.flip(np.sort(rec.numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# a = nlr.predict(np.zeros([100, 10379]), np.arange(100))\n",
    "b = model.predict(np.zeros([100, 10379]), np.arange(100))\n",
    "print(time.time() - start)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "19/19 [==============================] - 11s 553ms/step - loss: 0.2560 - accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 0s 23ms/step - loss: 0.1712 - accuracy: 0.0104\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 0s 23ms/step - loss: 0.1570 - accuracy: 0.0521\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 0s 25ms/step - loss: 0.1597 - accuracy: 0.0247\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 0s 24ms/step - loss: 0.1457 - accuracy: 0.0362\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 0s 19ms/step - loss: 0.1386 - accuracy: 0.0260\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 0s 20ms/step - loss: 0.1282 - accuracy: 0.0310\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 0s 20ms/step - loss: 0.1163 - accuracy: 0.0237\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 0s 20ms/step - loss: 0.1191 - accuracy: 0.0225\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 0s 21ms/step - loss: 0.1187 - accuracy: 0.0033\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 0.1050 - accuracy: 0.0294\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 0s 15ms/step - loss: 0.0945 - accuracy: 0.0328\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 0s 19ms/step - loss: 0.1052 - accuracy: 0.0260\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 0s 20ms/step - loss: 0.1089 - accuracy: 0.0164\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 0s 21ms/step - loss: 0.0957 - accuracy: 0.0132\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 0s 20ms/step - loss: 0.0899 - accuracy: 0.0277\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 0s 21ms/step - loss: 0.0918 - accuracy: 0.0197\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 0s 19ms/step - loss: 0.0831 - accuracy: 0.0121\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 0.0724 - accuracy: 0.0174\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 0s 22ms/step - loss: 0.0801 - accuracy: 0.0294\n"
     ]
    }
   ],
   "source": [
    "model = models.ConstraintAutoRec.ConstraintAutoRec(ml_small['dimensions'], epochs=20, novelty_weight=0.1, diversity_weight=0.1, latent_dim=64)\n",
    "model.train(utils.common.load_dataset(ml_small), ml_small['train']['records'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch nr 1 predicted\n",
      "Batch nr 2 predicted\n",
      "waiting for queue\n",
      "processing results\n",
      "Evaluating ConstraintAutoRec |============================================================>| 100.0% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7145099325337332,\n",
       " 'precision': 0.8471330878485008,\n",
       " 'recall': 0.7649243160110086,\n",
       " 'map@1': 0.05078125,\n",
       " 'map@5': 0.2159743923611111,\n",
       " 'map@10': 0.11500614442102072,\n",
       " 'diversity@5': 0.24681915309043226,\n",
       " 'diversity@10': 0.2627248113738829,\n",
       " 'epc@5': 0.7367946729294708,\n",
       " 'epc@10': 0.774178563936214,\n",
       " 'epd@5': 0.31932444495719237,\n",
       " 'name': 'ConstraintAutoRec',\n",
       " 'dimensions': 10381,\n",
       " 'latent_dims': 64,\n",
       " 'accuracy_weight': 1.0,\n",
       " 'novelty_weight': 0.1,\n",
       " 'diversity_weight': 0.1,\n",
       " 'epochs': 20,\n",
       " 'batch_size': 32,\n",
       " 'optimizer': 'adam'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev.evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.evaluate(nlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.flip(np.argsort(nlr.predict_single_user(80)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlr.model.user_embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable(initial_value=tf.random.normal([1]))\n",
    "b = tf.Variable(initial_value=tf.random.normal([1]))\n",
    "tf.stack([a,b],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_max(a, axis=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_2args(X,Y):\n",
    "    if X.doms == [] and Y.doms == []:\n",
    "        result = tf.concat([X,Y],axis=-1)\n",
    "        result.doms = []\n",
    "        return result,[X,Y]\n",
    "    X_Y = set(X.doms) - set(Y.doms)\n",
    "    Y_X = set(Y.doms) - set(X.doms)\n",
    "    eX = X\n",
    "    eX_doms = [x for x in X.doms]\n",
    "    for y in Y_X:\n",
    "        eX = tf.expand_dims(eX,0)\n",
    "        eX_doms = [y] + eX_doms\n",
    "    eY = Y\n",
    "    eY_doms = [y for y in Y.doms]\n",
    "    for x in X_Y:\n",
    "        eY = tf.expand_dims(eY,-2)\n",
    "        eY_doms.append(x)\n",
    "    perm_eY = []\n",
    "    for y in eY_doms:\n",
    "        perm_eY.append(eX_doms.index(y))\n",
    "    eY = tf.transpose(eY,perm=perm_eY + [len(perm_eY)])\n",
    "    mult_eX = [1]*(len(eX_doms)+1)\n",
    "    mult_eY = [1]*(len(eY_doms)+1)\n",
    "    for i in range(len(mult_eX)-1):\n",
    "        mult_eX[i] = tf.maximum(1,tf.math.floordiv(tf.shape(eY)[i],tf.shape(eX)[i]))\n",
    "        mult_eY[i] = tf.maximum(1,tf.math.floordiv(tf.shape(eX)[i],tf.shape(eY)[i]))\n",
    "    result1 = tf.tile(eX,mult_eX)\n",
    "    result2 = tf.tile(eY,mult_eY)\n",
    "    result = tf.concat([result1,result2],axis=-1)\n",
    "    result1.doms = eX_doms\n",
    "    result2.doms = eX_doms\n",
    "    result.doms = eX_doms\n",
    "    return result,[result1,result2]\n",
    "\n",
    "a.doms = [\"a\"]\n",
    "b.doms = [\"b\"]\n",
    "result, _ = cross_2args(a,b )\n",
    "r_and = tf.reduce_mean(result, axis=2, keepdims=True)\n",
    "print(a)\n",
    "print(b)\n",
    "print(result)\n",
    "print(r_and)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = (a,b )\n",
    "ax = [result.doms.index(var.doms[0]) for var in vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = tf.convert_to_tensor([300,100, 200], dtype=tf.int32)\n",
    "users = tf.convert_to_tensor(np.arange(5), dtype=tf.int32)\n",
    "users_1 = tf.expand_dims(tf.tile(users, [users.shape[0]]), axis=0)\n",
    "users_2 = tf.sort(users_1)\n",
    "tf.concat([users_1, users_2], axis=0)\n",
    "# tf.transpose(tf.reshape(repeated, [2, 5 * 4 // 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.reduce_mean(tf.convert_to_tensor(np.arange(24)))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
