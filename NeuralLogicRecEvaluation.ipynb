{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralLogicRec evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.NeuralLogicRec' from '/home/ec2-user/SageMaker/rs-ml-with-constraints/models/NeuralLogicRec.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils.common\n",
    "import evaluation\n",
    "import importlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import models.MatrixFactorization\n",
    "import models.BPR\n",
    "import models.ConstraintAutoRec\n",
    "import models.NeuralLogicRec\n",
    "import pandas as pd\n",
    "from models.NeuralLogicRec import item_cf, user_cf, diversity_constraint, Constraint, And, Or, Implies, Forall, Not, Equiv\n",
    "importlib.reload(utils.common)\n",
    "importlib.reload(evaluation)\n",
    "importlib.reload(models.NeuralLogicRec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ml = evaluation.Evaluation(utils.common.movie_lens)\n",
    "ml_evals = list()\n",
    "ev_test = evaluation.Evaluation(utils.common.ml_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = list()\n",
    "constraints.append(Constraint(weight=0.2, formula=item_cf))\n",
    "constraints.append(Constraint(weight=0.2, formula=user_cf))\n",
    "@tf.function\n",
    "def likes_equiv(model, outputs):\n",
    "    return Forall(Equiv(outputs['rec'], outputs['likes']))\n",
    "constraints.append(Constraint(weight=0.8, formula=likes_equiv))\n",
    "\n",
    "popularity = np.load(utils.common.movie_lens['train']['item_frequency'])\n",
    "pop = tf.convert_to_tensor(popularity.squeeze(), tf.float32)\n",
    "@tf.function\n",
    "def novelty_constraint(model, outputs):\n",
    "    return Forall(Implies(pop, Not(outputs['rec'])))\n",
    "constraints.append(Constraint(weight=0.3, formula=novelty_constraint))\n",
    "constraints.append(Constraint(weight=0.3, formula=diversity_constraint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models for MovieLens data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlr_ml = models.NeuralLogicRec.NLR(utils.common.movie_lens['user'], utils.common.movie_lens['dimensions'], epochs=1, constraints=constraints, mode='ae')\n",
    "nlr_history = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 Loss at step 2885: 0.1764, time: 530.687. Train accuracy 0.864, Validation accuracy 0.714\n",
      "{'accuracy': 0.7639793153936321, 'precision@5': 0.19583775282542704, 'recall@5': 0.08462963932068755, 'map@1': 0.23064262462967705, 'map@5': 0.13731892274118238, 'map@10': 0.11425578565504584, 'diversity@5': 0.1171658647277152, 'diversity@10': 0.12522683848654445, 'epc@5': 0.7473682398815811, 'epc@10': 0.7665372419631042, 'epd@5': 0.14907433103713885, 'coverage@1': 0.0633850303438975, 'coverage@5': 0.13226086118871014, 'coverage@10': 0.17454965802909161, 'name': 'NeuralLogicRec_AE_ML_6epochs', 'embedding_dim': 32, 'epochs_trained': 7, 'batch_size': 48, 'nr_hidden_layers': 3, 'nr_item_samples': 4096}\n",
      "Epoch #0 Loss at step 2885: 0.1841, time: 531.785. Train accuracy 0.866, Validation accuracy 0.713\n",
      "{'accuracy': 0.7704576561014852, 'precision@5': 0.19650341977250285, 'recall@5': 0.08311864163137603, 'map@1': 0.23539738853736147, 'map@5': 0.13850949327221226, 'map@10': 0.11447389975968524, 'diversity@5': 0.11543065751051688, 'diversity@10': 0.12355099000327129, 'epc@5': 0.7335864081342937, 'epc@10': 0.7548622182154197, 'epd@5': 0.14843277103739902, 'coverage@1': 0.062229072343704846, 'coverage@5': 0.12715538002119256, 'coverage@10': 0.17329737019554955, 'name': 'NeuralLogicRec_AE_ML_6epochs', 'embedding_dim': 32, 'epochs_trained': 8, 'batch_size': 48, 'nr_hidden_layers': 3, 'nr_item_samples': 4096}\n",
      "Epoch #0 Loss at step 2885: 0.2182, time: 530.011. Train accuracy 0.887, Validation accuracy 0.783\n",
      "{'accuracy': 0.7826366325726262, 'precision@5': 0.20158735964302696, 'recall@5': 0.08613573940145093, 'map@1': 0.24300501078965656, 'map@5': 0.14332825345736358, 'map@10': 0.11943181393183329, 'diversity@5': 0.11623865759085801, 'diversity@10': 0.12459845496481832, 'epc@5': 0.7312173052323402, 'epc@10': 0.7510611802904413, 'epd@5': 0.14938198260930313, 'coverage@1': 0.0601098160100183, 'coverage@5': 0.12860032752143338, 'coverage@10': 0.1755129563625855, 'name': 'NeuralLogicRec_AE_ML_6epochs', 'embedding_dim': 32, 'epochs_trained': 9, 'batch_size': 48, 'nr_hidden_layers': 3, 'nr_item_samples': 4096}\n",
      "Epoch #0 Loss at step 2885: 0.2042, time: 530.267. Train accuracy 0.865, Validation accuracy 0.759\n",
      "{'accuracy': 0.7639693317552099, 'precision@5': 0.19562561720493032, 'recall@5': 0.08295208734720372, 'map@1': 0.23386123404410958, 'map@5': 0.13767538779773153, 'map@10': 0.11379868638369954, 'diversity@5': 0.11681304527066369, 'diversity@10': 0.12463046685775994, 'epc@5': 0.7517227529752447, 'epc@10': 0.7688702575738514, 'epd@5': 0.1496643102364587, 'coverage@1': 0.06242173201040362, 'coverage@5': 0.13630671418938445, 'coverage@10': 0.18148540603024757, 'name': 'NeuralLogicRec_AE_ML_6epochs', 'embedding_dim': 32, 'epochs_trained': 10, 'batch_size': 48, 'nr_hidden_layers': 3, 'nr_item_samples': 4096}\n"
     ]
    }
   ],
   "source": [
    "for e in range(4):\n",
    "    nlr_ml.train(utils.common.load_dataset(utils.common.movie_lens), utils.common.movie_lens['train']['records'])\n",
    "    ev = eval_ml.evaluate_single_thread(nlr_ml)\n",
    "    print(ev)\n",
    "    nlr_history.append(ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlr_ml.additional_name = 'AE_ML_10_epochs'\n",
    "nlr_ml.save('../saved_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>coverage@1</th>\n",
       "      <th>coverage@10</th>\n",
       "      <th>coverage@5</th>\n",
       "      <th>diversity@10</th>\n",
       "      <th>diversity@5</th>\n",
       "      <th>embedding_dim</th>\n",
       "      <th>epc@10</th>\n",
       "      <th>epc@5</th>\n",
       "      <th>epd@5</th>\n",
       "      <th>epochs_trained</th>\n",
       "      <th>map@1</th>\n",
       "      <th>map@10</th>\n",
       "      <th>map@5</th>\n",
       "      <th>name</th>\n",
       "      <th>nr_hidden_layers</th>\n",
       "      <th>nr_item_samples</th>\n",
       "      <th>precision@5</th>\n",
       "      <th>recall@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.662211</td>\n",
       "      <td>48</td>\n",
       "      <td>0.018206</td>\n",
       "      <td>0.059724</td>\n",
       "      <td>0.039399</td>\n",
       "      <td>0.127913</td>\n",
       "      <td>0.120962</td>\n",
       "      <td>32</td>\n",
       "      <td>0.716507</td>\n",
       "      <td>0.688496</td>\n",
       "      <td>0.155785</td>\n",
       "      <td>1</td>\n",
       "      <td>0.156249</td>\n",
       "      <td>0.070051</td>\n",
       "      <td>0.084325</td>\n",
       "      <td>NeuralLogicRec_default</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.128166</td>\n",
       "      <td>0.057129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.741166</td>\n",
       "      <td>48</td>\n",
       "      <td>0.029284</td>\n",
       "      <td>0.090357</td>\n",
       "      <td>0.063192</td>\n",
       "      <td>0.123090</td>\n",
       "      <td>0.115783</td>\n",
       "      <td>32</td>\n",
       "      <td>0.734332</td>\n",
       "      <td>0.710480</td>\n",
       "      <td>0.152213</td>\n",
       "      <td>2</td>\n",
       "      <td>0.199627</td>\n",
       "      <td>0.098099</td>\n",
       "      <td>0.117074</td>\n",
       "      <td>NeuralLogicRec_default</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.171559</td>\n",
       "      <td>0.074401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.756827</td>\n",
       "      <td>48</td>\n",
       "      <td>0.043252</td>\n",
       "      <td>0.125710</td>\n",
       "      <td>0.091706</td>\n",
       "      <td>0.121741</td>\n",
       "      <td>0.114167</td>\n",
       "      <td>32</td>\n",
       "      <td>0.749312</td>\n",
       "      <td>0.729191</td>\n",
       "      <td>0.150313</td>\n",
       "      <td>3</td>\n",
       "      <td>0.223547</td>\n",
       "      <td>0.107418</td>\n",
       "      <td>0.129927</td>\n",
       "      <td>NeuralLogicRec_default</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.184960</td>\n",
       "      <td>0.079643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.752100</td>\n",
       "      <td>48</td>\n",
       "      <td>0.050958</td>\n",
       "      <td>0.151238</td>\n",
       "      <td>0.108660</td>\n",
       "      <td>0.123381</td>\n",
       "      <td>0.115557</td>\n",
       "      <td>32</td>\n",
       "      <td>0.759435</td>\n",
       "      <td>0.740621</td>\n",
       "      <td>0.149968</td>\n",
       "      <td>4</td>\n",
       "      <td>0.223291</td>\n",
       "      <td>0.107024</td>\n",
       "      <td>0.129669</td>\n",
       "      <td>NeuralLogicRec_default</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.186094</td>\n",
       "      <td>0.078803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.759215</td>\n",
       "      <td>48</td>\n",
       "      <td>0.055871</td>\n",
       "      <td>0.156632</td>\n",
       "      <td>0.115692</td>\n",
       "      <td>0.124262</td>\n",
       "      <td>0.116207</td>\n",
       "      <td>32</td>\n",
       "      <td>0.767957</td>\n",
       "      <td>0.753111</td>\n",
       "      <td>0.150008</td>\n",
       "      <td>5</td>\n",
       "      <td>0.228594</td>\n",
       "      <td>0.108817</td>\n",
       "      <td>0.132118</td>\n",
       "      <td>NeuralLogicRec_default</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.187850</td>\n",
       "      <td>0.078178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.782511</td>\n",
       "      <td>48</td>\n",
       "      <td>0.056642</td>\n",
       "      <td>0.162990</td>\n",
       "      <td>0.119256</td>\n",
       "      <td>0.123521</td>\n",
       "      <td>0.115019</td>\n",
       "      <td>32</td>\n",
       "      <td>0.754347</td>\n",
       "      <td>0.734855</td>\n",
       "      <td>0.148637</td>\n",
       "      <td>6</td>\n",
       "      <td>0.237702</td>\n",
       "      <td>0.114901</td>\n",
       "      <td>0.138578</td>\n",
       "      <td>NeuralLogicRec_default</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.195318</td>\n",
       "      <td>0.083840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.763979</td>\n",
       "      <td>48</td>\n",
       "      <td>0.063385</td>\n",
       "      <td>0.174550</td>\n",
       "      <td>0.132261</td>\n",
       "      <td>0.125227</td>\n",
       "      <td>0.117166</td>\n",
       "      <td>32</td>\n",
       "      <td>0.766537</td>\n",
       "      <td>0.747368</td>\n",
       "      <td>0.149074</td>\n",
       "      <td>7</td>\n",
       "      <td>0.230643</td>\n",
       "      <td>0.114256</td>\n",
       "      <td>0.137319</td>\n",
       "      <td>NeuralLogicRec_AE_ML_6epochs</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.195838</td>\n",
       "      <td>0.084630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.770458</td>\n",
       "      <td>48</td>\n",
       "      <td>0.062229</td>\n",
       "      <td>0.173297</td>\n",
       "      <td>0.127155</td>\n",
       "      <td>0.123551</td>\n",
       "      <td>0.115431</td>\n",
       "      <td>32</td>\n",
       "      <td>0.754862</td>\n",
       "      <td>0.733586</td>\n",
       "      <td>0.148433</td>\n",
       "      <td>8</td>\n",
       "      <td>0.235397</td>\n",
       "      <td>0.114474</td>\n",
       "      <td>0.138509</td>\n",
       "      <td>NeuralLogicRec_AE_ML_6epochs</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.196503</td>\n",
       "      <td>0.083119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.782637</td>\n",
       "      <td>48</td>\n",
       "      <td>0.060110</td>\n",
       "      <td>0.175513</td>\n",
       "      <td>0.128600</td>\n",
       "      <td>0.124598</td>\n",
       "      <td>0.116239</td>\n",
       "      <td>32</td>\n",
       "      <td>0.751061</td>\n",
       "      <td>0.731217</td>\n",
       "      <td>0.149382</td>\n",
       "      <td>9</td>\n",
       "      <td>0.243005</td>\n",
       "      <td>0.119432</td>\n",
       "      <td>0.143328</td>\n",
       "      <td>NeuralLogicRec_AE_ML_6epochs</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.201587</td>\n",
       "      <td>0.086136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.763969</td>\n",
       "      <td>48</td>\n",
       "      <td>0.062422</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.136307</td>\n",
       "      <td>0.124630</td>\n",
       "      <td>0.116813</td>\n",
       "      <td>32</td>\n",
       "      <td>0.768870</td>\n",
       "      <td>0.751723</td>\n",
       "      <td>0.149664</td>\n",
       "      <td>10</td>\n",
       "      <td>0.233861</td>\n",
       "      <td>0.113799</td>\n",
       "      <td>0.137675</td>\n",
       "      <td>NeuralLogicRec_AE_ML_6epochs</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.195626</td>\n",
       "      <td>0.082952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  batch_size  coverage@1  coverage@10  coverage@5  diversity@10  \\\n",
       "0  0.662211          48    0.018206     0.059724    0.039399      0.127913   \n",
       "1  0.741166          48    0.029284     0.090357    0.063192      0.123090   \n",
       "2  0.756827          48    0.043252     0.125710    0.091706      0.121741   \n",
       "3  0.752100          48    0.050958     0.151238    0.108660      0.123381   \n",
       "4  0.759215          48    0.055871     0.156632    0.115692      0.124262   \n",
       "5  0.782511          48    0.056642     0.162990    0.119256      0.123521   \n",
       "6  0.763979          48    0.063385     0.174550    0.132261      0.125227   \n",
       "7  0.770458          48    0.062229     0.173297    0.127155      0.123551   \n",
       "8  0.782637          48    0.060110     0.175513    0.128600      0.124598   \n",
       "9  0.763969          48    0.062422     0.181485    0.136307      0.124630   \n",
       "\n",
       "   diversity@5  embedding_dim    epc@10     epc@5     epd@5  epochs_trained  \\\n",
       "0     0.120962             32  0.716507  0.688496  0.155785               1   \n",
       "1     0.115783             32  0.734332  0.710480  0.152213               2   \n",
       "2     0.114167             32  0.749312  0.729191  0.150313               3   \n",
       "3     0.115557             32  0.759435  0.740621  0.149968               4   \n",
       "4     0.116207             32  0.767957  0.753111  0.150008               5   \n",
       "5     0.115019             32  0.754347  0.734855  0.148637               6   \n",
       "6     0.117166             32  0.766537  0.747368  0.149074               7   \n",
       "7     0.115431             32  0.754862  0.733586  0.148433               8   \n",
       "8     0.116239             32  0.751061  0.731217  0.149382               9   \n",
       "9     0.116813             32  0.768870  0.751723  0.149664              10   \n",
       "\n",
       "      map@1    map@10     map@5                          name  \\\n",
       "0  0.156249  0.070051  0.084325        NeuralLogicRec_default   \n",
       "1  0.199627  0.098099  0.117074        NeuralLogicRec_default   \n",
       "2  0.223547  0.107418  0.129927        NeuralLogicRec_default   \n",
       "3  0.223291  0.107024  0.129669        NeuralLogicRec_default   \n",
       "4  0.228594  0.108817  0.132118        NeuralLogicRec_default   \n",
       "5  0.237702  0.114901  0.138578        NeuralLogicRec_default   \n",
       "6  0.230643  0.114256  0.137319  NeuralLogicRec_AE_ML_6epochs   \n",
       "7  0.235397  0.114474  0.138509  NeuralLogicRec_AE_ML_6epochs   \n",
       "8  0.243005  0.119432  0.143328  NeuralLogicRec_AE_ML_6epochs   \n",
       "9  0.233861  0.113799  0.137675  NeuralLogicRec_AE_ML_6epochs   \n",
       "\n",
       "   nr_hidden_layers  nr_item_samples  precision@5  recall@5  \n",
       "0                 3             4096     0.128166  0.057129  \n",
       "1                 3             4096     0.171559  0.074401  \n",
       "2                 3             4096     0.184960  0.079643  \n",
       "3                 3             4096     0.186094  0.078803  \n",
       "4                 3             4096     0.187850  0.078178  \n",
       "5                 3             4096     0.195318  0.083840  \n",
       "6                 3             4096     0.195838  0.084630  \n",
       "7                 3             4096     0.196503  0.083119  \n",
       "8                 3             4096     0.201587  0.086136  \n",
       "9                 3             4096     0.195626  0.082952  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(nlr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 Loss at step 2540: 0.2994, time: 761.080. Train accuracy 0.821, Validation accuracy 0.681\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 Loss at step 2885: 0.3053, time: 860.654. Train accuracy 0.802, Validation accuracy 0.656\n",
      "Epoch #0 Loss at step 2885: 0.2906, time: 850.057. Train accuracy 0.817, Validation accuracy 0.693\n",
      "Epoch #0 Loss at step 2885: 0.2675, time: 849.426. Train accuracy 0.808, Validation accuracy 0.619\n",
      "Epoch #0 Loss at step 2885: 0.2895, time: 847.871. Train accuracy 0.817, Validation accuracy 0.678\n",
      "Epoch #0 Loss at step 2885: 0.2747, time: 846.828. Train accuracy 0.818, Validation accuracy 0.706\n",
      "Epoch #0 Loss at step 2885: 0.3012, time: 846.495. Train accuracy 0.820, Validation accuracy 0.685\n",
      "Epoch #0 Loss at step 2885: 0.2642, time: 845.379. Train accuracy 0.822, Validation accuracy 0.680\n",
      "Epoch #0 Loss at step 2885: 0.2683, time: 846.312. Train accuracy 0.803, Validation accuracy 0.646\n",
      "Epoch #0 Loss at step 2885: 0.2453, time: 845.316. Train accuracy 0.831, Validation accuracy 0.734\n",
      "Epoch #0 Loss at step 2885: 0.2565, time: 845.620. Train accuracy 0.801, Validation accuracy 0.663\n",
      "Batch nr 214 predicted\r"
     ]
    }
   ],
   "source": [
    "nlr_ml_nn = models.NeuralLogicRec.NLR(utils.common.movie_lens['user'], utils.common.movie_lens['dimensions'], epochs=1, constraints=constraints, mode='nn', name='NN_ML_10_epochs')\n",
    "nlr_history_nn = list()\n",
    "for e in range(10):\n",
    "    nlr_ml_nn.train(utils.common.load_dataset(utils.common.movie_lens), utils.common.movie_lens['train']['records'])\n",
    "    nlr_history_nn.append(eval_ml.evaluate_single_thread(nlr_ml_nn))\n",
    "nlr_ml_nn.save('../saved_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0912 08:16:38.986819 140244721764160 deprecation.py:323] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 Loss at step 5770: 0.1123, time: 347.705. Train P@1 0.466 P@5 0.431, Eval P@1 0.022 P@5 0.025\n",
      "Epoch #1 Loss at step 5770: 0.1003, time: 334.177. Train P@1 0.540 P@5 0.481, Eval P@1 0.024 P@5 0.027\n",
      "Epoch #1 Loss at step 5770: 0.0971, time: 335.844. Train P@1 0.542 P@5 0.485, Eval P@1 0.028 P@5 0.029\n",
      "Epoch #1 Loss at step 5770: 0.0955, time: 331.848. Train P@1 0.516 P@5 0.491, Eval P@1 0.037 P@5 0.027\n",
      "Epoch #1 Loss at step 5770: 0.0944, time: 333.180. Train P@1 0.573 P@5 0.506, Eval P@1 0.025 P@5 0.029\n",
      "Epoch #1 Loss at step 5770: 0.0937, time: 334.129. Train P@1 0.554 P@5 0.501, Eval P@1 0.032 P@5 0.027\n",
      "Epoch #1 Loss at step 5770: 0.0931, time: 333.900. Train P@1 0.524 P@5 0.505, Eval P@1 0.029 P@5 0.028\n",
      "Epoch #1 Loss at step 5770: 0.0926, time: 335.217. Train P@1 0.565 P@5 0.518, Eval P@1 0.031 P@5 0.031\n",
      "Epoch #1 Loss at step 5770: 0.0923, time: 328.943. Train P@1 0.528 P@5 0.501, Eval P@1 0.029 P@5 0.027\n",
      "Epoch #1 Loss at step 5770: 0.0921, time: 329.253. Train P@1 0.520 P@5 0.486, Eval P@1 0.026 P@5 0.027\n",
      "Batch nr 215 predicted\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nlr_ml_nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5591e2144fd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnlr_history_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mev_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlr_history_v2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../evals/V2_ML_10_epochs.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnlr_ml_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../saved_models/NLR'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nlr_ml_nn' is not defined"
     ]
    }
   ],
   "source": [
    "nlr_ml_v2 = models.NeuralLogicRec.NLR(utils.common.movie_lens['user'], utils.common.movie_lens['dimensions'], epochs=1, constraints=constraints, mode='v2', name='V2_ML_10_epochs')\n",
    "nlr_history_v2 = list()\n",
    "for e in range(10):\n",
    "    nlr_ml_v2.train(utils.common.load_dataset(utils.common.movie_lens), utils.common.movie_lens['train']['records'])\n",
    "    ev_result = eval_ml.evaluate_single_thread(nlr_ml_v2)\n",
    "    nlr_history_v2.append(ev_result)\n",
    "    pd.DataFrame(nlr_history_v2).to_csv('../evals/V2_ML_10_epochs.csv')\n",
    "nlr_ml_nn.save('../saved_models/NLR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(nlr_history_v2).to_csv('../evals/V2_ML_10_epochs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of model architectures on MovieLens data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models for MSD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_msd = evaluation.Evaluation(utils.common.msd)\n",
    "# nlr_msd_ae = models.NeuralLogicRec.NLR(utils.common.msd['user'], utils.common.msd['dimensions'], epochs=1, constraints=constraints, mode='ae', name='AE_MSD_10_epochs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_msd = list()\n",
    "constraints_msd.append(Constraint(weight=0.2, formula=item_cf))\n",
    "constraints_msd.append(Constraint(weight=0.2, formula=user_cf))\n",
    "@tf.function\n",
    "def likes_equiv(model, outputs):\n",
    "    return Forall(Equiv(outputs['rec'], outputs['likes']))\n",
    "constraints_msd.append(Constraint(weight=0.8, formula=likes_equiv))\n",
    "\n",
    "popularity_msd = np.load(utils.common.msd['train']['item_frequency'])\n",
    "popularity_msd = tf.convert_to_tensor(popularity_msd.squeeze(), tf.float32)\n",
    "@tf.function\n",
    "def novelty_constraint(model, outputs):\n",
    "    return Forall(Implies(popularity_msd, Not(outputs['rec'])))\n",
    "constraints_msd.append(Constraint(weight=0.3, formula=novelty_constraint))\n",
    "constraints_msd.append(Constraint(weight=0.3, formula=diversity_constraint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 Loss at step 2042: 0.3104, time: 275.357. Train accuracy 0.703, Validation accuracy 0.400\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 Loss at step 2457: 0.3340, time: 328.467. Train accuracy 0.724, Validation accuracy 0.465\n",
      "Epoch #0 Loss at step 2457: 0.2783, time: 320.835. Train accuracy 0.788, Validation accuracy 0.575\n",
      "Epoch #0 Loss at step 2457: 0.2890, time: 322.371. Train accuracy 0.777, Validation accuracy 0.585\n",
      "Epoch #0 Loss at step 2457: 0.2493, time: 321.966. Train accuracy 0.787, Validation accuracy 0.538\n",
      "Epoch #0 Loss at step 2457: 0.2530, time: 321.648. Train accuracy 0.797, Validation accuracy 0.628\n",
      "Epoch #0 Loss at step 2457: 0.2628, time: 321.805. Train accuracy 0.783, Validation accuracy 0.490\n",
      "Epoch #0 Loss at step 2457: 0.2583, time: 321.144. Train accuracy 0.799, Validation accuracy 0.614\n",
      "Epoch #0 Loss at step 2457: 0.2412, time: 322.453. Train accuracy 0.813, Validation accuracy 0.707\n",
      "Epoch #0 Loss at step 2457: 0.2473, time: 321.494. Train accuracy 0.790, Validation accuracy 0.601\n",
      "Epoch #0 Loss at step 2457: 0.2455, time: 322.070. Train accuracy 0.799, Validation accuracy 0.557\n",
      "Batch nr 186 predicted\r"
     ]
    }
   ],
   "source": [
    "nlr_history_msd_ae = list()\n",
    "for e in range(10):\n",
    "    nlr_msd_ae.train(utils.common.load_dataset(utils.common.msd), utils.common.msd['train']['records'])\n",
    "    nlr_history_msd_ae.append(eval_msd.evaluate_single_thread(nlr_msd_ae))\n",
    "nlr_msd_ae.save('../saved_models')\n",
    "pd.DataFrame(nlr_history_msd_ae).to_csv('../evals/AE_MSD_10_epochs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 Loss at step 2027: 0.4726, time: 418.433. Train accuracy 0.682, Validation accuracy 0.422\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 Loss at step 2457: 0.4523, time: 503.762. Train accuracy 0.700, Validation accuracy 0.533\n",
      "Epoch #0 Loss at step 2457: 0.4498, time: 493.903. Train accuracy 0.684, Validation accuracy 0.423\n",
      "Epoch #0 Loss at step 2457: 0.4354, time: 493.372. Train accuracy 0.679, Validation accuracy 0.462\n",
      "Epoch #0 Loss at step 2457: 0.3933, time: 494.941. Train accuracy 0.703, Validation accuracy 0.416\n",
      "Epoch #0 Loss at step 2457: 0.3980, time: 496.310. Train accuracy 0.686, Validation accuracy 0.410\n",
      "Epoch #0 Loss at step 2457: 0.3974, time: 494.934. Train accuracy 0.697, Validation accuracy 0.468\n",
      "Epoch #0 Loss at step 2457: 0.4141, time: 497.709. Train accuracy 0.696, Validation accuracy 0.504\n",
      "Epoch #0 Loss at step 2457: 0.4180, time: 497.081. Train accuracy 0.717, Validation accuracy 0.545\n",
      "Epoch #0 Loss at step 2457: 0.4069, time: 493.295. Train accuracy 0.750, Validation accuracy 0.579\n",
      "Epoch #0 Loss at step 2457: 0.3956, time: 493.245. Train accuracy 0.713, Validation accuracy 0.531\n",
      "Batch nr 186 predicted\r"
     ]
    }
   ],
   "source": [
    "nlr_msd_nn = models.NeuralLogicRec.NLR(utils.common.msd['user'], utils.common.msd['dimensions'], epochs=1, constraints=constraints, mode='nn', name='NN_MSD_10_epochs')\n",
    "nlr_history_msd_nn = list()\n",
    "for e in range(10):\n",
    "    nlr_msd_nn.train(utils.common.load_dataset(utils.common.msd), utils.common.msd['train']['records'])\n",
    "    nlr_history_msd_nn.append(eval_msd.evaluate_single_thread(nlr_msd_nn))\n",
    "nlr_msd_nn.save('../saved_models')\n",
    "pd.DataFrame(nlr_history_msd_nn).to_csv('../evals/NN_MSD_10_epochs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 Loss at step 4915: 0.1596, time: 258.066. Train P@1 0.252 P@5 0.246, Eval P@1 0.014 P@5 0.013\n",
      "Epoch #1 Loss at step 4915: 0.1342, time: 251.878. Train P@1 0.352 P@5 0.307, Eval P@1 0.018 P@5 0.017\n",
      "Epoch #1 Loss at step 4915: 0.1282, time: 255.118. Train P@1 0.330 P@5 0.302, Eval P@1 0.022 P@5 0.019\n",
      "Epoch #1 Loss at step 4915: 0.1249, time: 253.772. Train P@1 0.338 P@5 0.312, Eval P@1 0.017 P@5 0.014\n",
      "Epoch #1 Loss at step 4915: 0.1228, time: 251.704. Train P@1 0.338 P@5 0.317, Eval P@1 0.019 P@5 0.016\n",
      "Epoch #1 Loss at step 4915: 0.1213, time: 250.724. Train P@1 0.361 P@5 0.319, Eval P@1 0.026 P@5 0.019\n",
      "Epoch #1 Loss at step 4915: 0.1203, time: 251.327. Train P@1 0.349 P@5 0.314, Eval P@1 0.018 P@5 0.015\n",
      "Epoch #1 Loss at step 4915: 0.1195, time: 252.279. Train P@1 0.370 P@5 0.325, Eval P@1 0.023 P@5 0.019\n",
      "Epoch #1 Loss at step 4915: 0.1188, time: 250.268. Train P@1 0.344 P@5 0.322, Eval P@1 0.021 P@5 0.018\n",
      "Epoch #1 Loss at step 4915: 0.1183, time: 254.450. Train P@1 0.354 P@5 0.325, Eval P@1 0.019 P@5 0.021\n",
      "Batch nr 185 predicted\r"
     ]
    }
   ],
   "source": [
    "nlr_msd_v2 = models.NeuralLogicRec.NLR(utils.common.msd['user'], utils.common.msd['dimensions'], epochs=1, constraints=constraints_msd, mode='v2', name='V2_MSD_10_epochs')\n",
    "nlr_history_msd_v2 = list()\n",
    "for e in range(10):\n",
    "    nlr_msd_v2.train(utils.common.load_dataset(utils.common.msd), utils.common.msd['train']['records'])\n",
    "    nlr_history_msd_v2.append(eval_msd.evaluate_single_thread(nlr_msd_v2))\n",
    "    pd.DataFrame(nlr_history_msd_v2).to_csv('../evals/V2_MSD_10_epochs.csv')\n",
    "nlr_msd_v2.save('../saved_models/NLR')\n",
    "pd.DataFrame(nlr_history_msd_v2).to_csv('../evals/V2_MSD_10_epochs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constraint evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_constraints = list()\n",
    "# base_constraints.append(Constraint(weight=0.25, formula=item_cf))\n",
    "# base_constraints.append(Constraint(weight=0.75, formula=user_cf))\n",
    "@tf.function\n",
    "def likes_equiv(model, outputs):\n",
    "    return Forall(Equiv(outputs['rec'], outputs['likes']))\n",
    "base_constraints.append(Constraint(weight=0.95, formula=likes_equiv))\n",
    "@tf.function\n",
    "def novelty_constraint(model, outputs):\n",
    "    return Forall(Implies(outputs['popular'], Not(outputs['rec'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diversity_evaluation(dataset, ds_name):\n",
    "    diversity_weights = [0.5, 1.0, 2.0, 4.0]\n",
    "    for div_weight in diversity_weights:\n",
    "        evals = list()\n",
    "        div_constraint = Constraint(weight=div_weight, formula=diversity_constraint)\n",
    "        diversity_model = models.NeuralLogicRec.NLR(dataset['user'], dataset['dimensions'], epochs=1, constraints=[div_constraint] + base_constraints, mode='v2', name=ds_name + '_div_v2_' + str(div_weight))\n",
    "        for e in range(5):\n",
    "            diversity_model.train(utils.common.load_dataset(dataset), dataset['train']['records'])\n",
    "            if ds_name == 'MSD':\n",
    "                evals.append(eval_msd.evaluate_single_thread(diversity_model))\n",
    "            else:\n",
    "                evals.append(eval_ml.evaluate_single_thread(diversity_model))\n",
    "        diversity_model.model._distribution_stragey = None\n",
    "        diversity_model.save('../saved_models/NLR')\n",
    "        pd.DataFrame(evals).to_csv('../evals/' + diversity_model.additional_name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0906 15:39:30.339435 140675917379392 deprecation.py:323] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 Loss at step 5770: 0.1144, time: 320.540. Train P@1 0.468 P@5 0.439, Eval P@1 0.033 P@5 0.025\n",
      "Epoch #2 Loss at step 5770: 0.1012, time: 312.830. Train P@1 0.517 P@5 0.470, Eval P@1 0.029 P@5 0.029\n",
      "Epoch #3 Loss at step 5770: 0.0981, time: 311.357. Train P@1 0.549 P@5 0.495, Eval P@1 0.026 P@5 0.025\n",
      "Epoch #4 Loss at step 5770: 0.0965, time: 313.219. Train P@1 0.532 P@5 0.503, Eval P@1 0.032 P@5 0.027\n",
      "Epoch #5 Loss at step 5770: 0.0956, time: 312.860. Train P@1 0.532 P@5 0.495, Eval P@1 0.030 P@5 0.028\n"
     ]
    }
   ],
   "source": [
    "ml_pretrain = models.NeuralLogicRec.NLR(utils.common.movie_lens['user'], utils.common.movie_lens['dimensions'], epochs=5, constraints=base_constraints, mode='v2', name='pretrain_v2')\n",
    "ml_pretrain.model._distribution_strategy = None\n",
    "ml_pretrain.train(utils.common.load_dataset(utils.common.movie_lens), utils.common.movie_lens['train']['records'])\n",
    "ml_pretrain.save('../saved_models/NLR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 Loss at step 4915: 0.1634, time: 239.246. Train P@1 0.272 P@5 0.251, Eval P@1 0.018 P@5 0.015\n",
      "Epoch #1 Loss at step 4915: 0.1378, time: 236.473. Train P@1 0.350 P@5 0.310, Eval P@1 0.023 P@5 0.020\n",
      "Epoch #1 Loss at step 4915: 0.1319, time: 238.980. Train P@1 0.333 P@5 0.311, Eval P@1 0.018 P@5 0.018\n",
      "Epoch #1 Loss at step 4915: 0.1285, time: 237.611. Train P@1 0.363 P@5 0.326, Eval P@1 0.020 P@5 0.018\n",
      "Epoch #1 Loss at step 4915: 0.1263, time: 232.530. Train P@1 0.367 P@5 0.321, Eval P@1 0.013 P@5 0.018\n",
      "Epoch #1 Loss at step 4915: 0.1632, time: 251.003. Train P@1 0.268 P@5 0.245, Eval P@1 0.011 P@5 0.016\n",
      "Epoch #1 Loss at step 4915: 0.1395, time: 247.596. Train P@1 0.322 P@5 0.290, Eval P@1 0.026 P@5 0.020\n",
      "Epoch #1 Loss at step 4915: 0.1343, time: 248.698. Train P@1 0.362 P@5 0.314, Eval P@1 0.018 P@5 0.016\n",
      "Epoch #1 Loss at step 4915: 0.1316, time: 249.624. Train P@1 0.363 P@5 0.314, Eval P@1 0.018 P@5 0.016\n",
      "Epoch #1 Loss at step 4915: 0.1299, time: 243.345. Train P@1 0.362 P@5 0.317, Eval P@1 0.027 P@5 0.018\n",
      "Epoch #1 Loss at step 4915: 0.1646, time: 256.897. Train P@1 0.281 P@5 0.241, Eval P@1 0.015 P@5 0.014\n",
      "Epoch #1 Loss at step 4915: 0.1397, time: 251.356. Train P@1 0.347 P@5 0.306, Eval P@1 0.018 P@5 0.015\n",
      "Epoch #1 Loss at step 4915: 0.1338, time: 246.327. Train P@1 0.345 P@5 0.315, Eval P@1 0.023 P@5 0.020\n",
      "Epoch #1 Loss at step 4915: 0.1307, time: 250.626. Train P@1 0.373 P@5 0.331, Eval P@1 0.027 P@5 0.018\n",
      "Epoch #1 Loss at step 4915: 0.1288, time: 248.470. Train P@1 0.365 P@5 0.336, Eval P@1 0.026 P@5 0.017\n",
      "Epoch #1 Loss at step 4915: 0.1677, time: 252.099. Train P@1 0.257 P@5 0.214, Eval P@1 0.014 P@5 0.012\n",
      "Epoch #1 Loss at step 4915: 0.1399, time: 251.098. Train P@1 0.309 P@5 0.288, Eval P@1 0.018 P@5 0.017\n",
      "Epoch #1 Loss at step 4915: 0.1338, time: 248.282. Train P@1 0.312 P@5 0.289, Eval P@1 0.018 P@5 0.014\n",
      "Epoch #1 Loss at step 928: 0.1326, time: 47.524. Train P@1 0.292 P@5 0.310, Eval P@1 0.021 P@5 0.018\r"
     ]
    }
   ],
   "source": [
    "ml_pretrain.model._distribution_strategy = None\n",
    "diversity_evaluation(utils.common.msd, 'MSD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "msd_pretrain = models.NeuralLogicRec.NLR(utils.common.msd['user'], utils.common.msd['dimensions'], epochs=5, constraints=base_constraints, mode='ae', name='msd_pretrain')\n",
    "# msd_pretrain.train(utils.common.load_dataset(utils.common.msd), utils.common.msd['train']['records'])\n",
    "msd_pretrain.model._distribution_stragey = None\n",
    "# msd_pretrain.save('../saved_models/NLR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 Loss at step 2457: 0.2512, time: 343.148. Train accuracy 0.817, Validation accuracy 0.724\n",
      "Epoch #0 Loss at step 2457: 0.2371, time: 332.964. Train accuracy 0.819, Validation accuracy 0.659\n",
      "Epoch #0 Loss at step 2457: 0.2577, time: 335.278. Train accuracy 0.838, Validation accuracy 0.700\n",
      "Epoch #0 Loss at step 2457: 0.2184, time: 335.431. Train accuracy 0.814, Validation accuracy 0.686\n",
      "Epoch #0 Loss at step 2457: 0.2408, time: 336.831. Train accuracy 0.814, Validation accuracy 0.646\n",
      "Epoch #0 Loss at step 2457: 0.2331, time: 336.646. Train accuracy 0.837, Validation accuracy 0.720\n",
      "Epoch #0 Loss at step 2457: 0.2579, time: 335.109. Train accuracy 0.815, Validation accuracy 0.650\n",
      "Epoch #0 Loss at step 2457: 0.2229, time: 333.546. Train accuracy 0.803, Validation accuracy 0.685\n",
      "Batch nr 186 predicted\r"
     ]
    }
   ],
   "source": [
    "msd_pretrain.model._distribution_strategy = None\n",
    "constraint_evaluation(utils.common.msd, 'MSD', msd_pretrain, 'NeuralLogicRec_msd_pretrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def novelty_evalution(dataset, ds_name, nov_constraint_formula):\n",
    "    novelty_weights = [0.1, 0.25, 0.5, 1.0]\n",
    "    for nov_weight in novelty_weights:\n",
    "        evals = list()\n",
    "        nov_constraint = Constraint(weight=nov_weight, formula=nov_constraint_formula)\n",
    "        nov_model = models.NeuralLogicRec.NLR(dataset['user'], dataset['dimensions'], epochs=1, constraints=[nov_constraint] + base_constraints, mode='v2', name=ds_name + '_nov_v2_' + str(nov_weight))\n",
    "        for e in range(5):\n",
    "            nov_model.train(utils.common.load_dataset(dataset), dataset['train']['records'])\n",
    "            if ds_name == 'MSD':\n",
    "                evals.append(eval_msd.evaluate_single_thread(nov_model))\n",
    "            else:\n",
    "                evals.append(eval_ml.evaluate_single_thread(nov_model))\n",
    "        nov_model.model._distribution_stragey = None\n",
    "        nov_model.save('../saved_models/NLR')\n",
    "        pd.DataFrame(evals).to_csv('../evals/' + nov_model.additional_name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 Loss at step 5770: 0.1076, time: 135.075. Train P@1 0.377 P@5 0.368, Eval P@1 0.023 P@5 0.024\n",
      "Epoch #1 Loss at step 5770: 0.0965, time: 132.967. Train P@1 0.477 P@5 0.433, Eval P@1 0.026 P@5 0.026\n",
      "Epoch #1 Loss at step 5770: 0.0932, time: 135.761. Train P@1 0.507 P@5 0.463, Eval P@1 0.028 P@5 0.029\n",
      "Epoch #1 Loss at step 5770: 0.0913, time: 132.487. Train P@1 0.506 P@5 0.469, Eval P@1 0.039 P@5 0.026\n",
      "Epoch #1 Loss at step 5770: 0.0901, time: 132.311. Train P@1 0.503 P@5 0.484, Eval P@1 0.037 P@5 0.026\n",
      "Epoch #1 Loss at step 5770: 0.1081, time: 134.935. Train P@1 0.417 P@5 0.366, Eval P@1 0.027 P@5 0.025\n",
      "Epoch #1 Loss at step 5770: 0.0966, time: 138.114. Train P@1 0.500 P@5 0.451, Eval P@1 0.028 P@5 0.027\n",
      "Epoch #1 Loss at step 5770: 0.0934, time: 133.213. Train P@1 0.500 P@5 0.473, Eval P@1 0.034 P@5 0.025\n",
      "Epoch #1 Loss at step 5770: 0.0917, time: 132.529. Train P@1 0.519 P@5 0.494, Eval P@1 0.036 P@5 0.026\n",
      "Epoch #1 Loss at step 5770: 0.0903, time: 132.641. Train P@1 0.542 P@5 0.481, Eval P@1 0.029 P@5 0.027\n",
      "Epoch #1 Loss at step 5770: 0.1088, time: 133.846. Train P@1 0.406 P@5 0.369, Eval P@1 0.021 P@5 0.019\n",
      "Epoch #1 Loss at step 5770: 0.0972, time: 134.281. Train P@1 0.481 P@5 0.433, Eval P@1 0.027 P@5 0.028\n",
      "Epoch #1 Loss at step 5770: 0.0937, time: 133.953. Train P@1 0.543 P@5 0.472, Eval P@1 0.030 P@5 0.029\n",
      "Epoch #1 Loss at step 5770: 0.0919, time: 133.502. Train P@1 0.514 P@5 0.486, Eval P@1 0.022 P@5 0.027\n",
      "Epoch #1 Loss at step 5770: 0.0907, time: 134.347. Train P@1 0.514 P@5 0.483, Eval P@1 0.021 P@5 0.026\n",
      "Epoch #1 Loss at step 5770: 0.1080, time: 134.813. Train P@1 0.343 P@5 0.312, Eval P@1 0.020 P@5 0.019\n",
      "Epoch #1 Loss at step 5770: 0.0965, time: 134.958. Train P@1 0.445 P@5 0.419, Eval P@1 0.024 P@5 0.021\n",
      "Epoch #1 Loss at step 5770: 0.0934, time: 133.377. Train P@1 0.476 P@5 0.439, Eval P@1 0.024 P@5 0.023\n",
      "Epoch #1 Loss at step 5770: 0.0916, time: 134.318. Train P@1 0.480 P@5 0.432, Eval P@1 0.019 P@5 0.026\n",
      "Epoch #1 Loss at step 5770: 0.0906, time: 134.336. Train P@1 0.478 P@5 0.428, Eval P@1 0.027 P@5 0.027\n",
      "Batch nr 215 predicted\r"
     ]
    }
   ],
   "source": [
    "popularity = np.load(utils.common.movie_lens['train']['item_frequency'])\n",
    "pop = tf.convert_to_tensor(popularity.squeeze(), tf.float32)\n",
    "@tf.function\n",
    "def ml_novelty_constraint(model, outputs):\n",
    "    return Forall(Implies(pop, Not(outputs['rec'])))\n",
    "novelty_evalution(utils.common.movie_lens, 'ML', ml_novelty_constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 Loss at step 4915: 0.1539, time: 114.268. Train P@1 0.221 P@5 0.204, Eval P@1 0.012 P@5 0.012\n",
      "Epoch #1 Loss at step 4915: 0.1272, time: 110.984. Train P@1 0.291 P@5 0.253, Eval P@1 0.013 P@5 0.015\n",
      "Epoch #1 Loss at step 4915: 0.1202, time: 110.944. Train P@1 0.318 P@5 0.287, Eval P@1 0.032 P@5 0.020\n",
      "Epoch #1 Loss at step 4915: 0.1160, time: 113.201. Train P@1 0.341 P@5 0.296, Eval P@1 0.019 P@5 0.016\n",
      "Epoch #1 Loss at step 4915: 0.1134, time: 112.571. Train P@1 0.363 P@5 0.315, Eval P@1 0.018 P@5 0.016\n",
      "Epoch #1 Loss at step 4915: 0.1518, time: 110.555. Train P@1 0.223 P@5 0.203, Eval P@1 0.013 P@5 0.013\n",
      "Epoch #1 Loss at step 4915: 0.1265, time: 111.172. Train P@1 0.313 P@5 0.277, Eval P@1 0.014 P@5 0.016\n",
      "Epoch #1 Loss at step 4915: 0.1202, time: 112.658. Train P@1 0.324 P@5 0.282, Eval P@1 0.013 P@5 0.015\n",
      "Epoch #1 Loss at step 4915: 0.1172, time: 113.763. Train P@1 0.307 P@5 0.280, Eval P@1 0.019 P@5 0.014\n",
      "Epoch #1 Loss at step 4915: 0.1153, time: 113.572. Train P@1 0.297 P@5 0.271, Eval P@1 0.015 P@5 0.015\n",
      "Epoch #1 Loss at step 4915: 0.1507, time: 115.723. Train P@1 0.235 P@5 0.213, Eval P@1 0.018 P@5 0.015\n",
      "Epoch #1 Loss at step 4915: 0.1269, time: 110.060. Train P@1 0.271 P@5 0.254, Eval P@1 0.013 P@5 0.014\n",
      "Epoch #1 Loss at step 4915: 0.1203, time: 111.690. Train P@1 0.310 P@5 0.284, Eval P@1 0.021 P@5 0.015\n",
      "Epoch #1 Loss at step 4915: 0.1164, time: 116.757. Train P@1 0.318 P@5 0.290, Eval P@1 0.018 P@5 0.015\n",
      "Epoch #1 Loss at step 4915: 0.1137, time: 112.856. Train P@1 0.304 P@5 0.287, Eval P@1 0.023 P@5 0.017\n",
      "Epoch #1 Loss at step 4915: 0.1528, time: 113.765. Train P@1 0.227 P@5 0.192, Eval P@1 0.010 P@5 0.010\n",
      "Epoch #1 Loss at step 4915: 0.1259, time: 112.855. Train P@1 0.262 P@5 0.241, Eval P@1 0.007 P@5 0.012\n",
      "Epoch #1 Loss at step 4915: 0.1198, time: 113.347. Train P@1 0.309 P@5 0.279, Eval P@1 0.017 P@5 0.015\n",
      "Epoch #1 Loss at step 4915: 0.1162, time: 112.687. Train P@1 0.305 P@5 0.286, Eval P@1 0.018 P@5 0.016\n",
      "Epoch #1 Loss at step 4915: 0.1137, time: 112.316. Train P@1 0.302 P@5 0.276, Eval P@1 0.020 P@5 0.017\n",
      "Batch nr 185 predicted\r"
     ]
    }
   ],
   "source": [
    "popularity_msd = np.load(utils.common.msd['train']['item_frequency'])\n",
    "popularity_msd = tf.convert_to_tensor(popularity_msd.squeeze(), tf.float32)\n",
    "@tf.function\n",
    "def msd_novelty_constraint(model, outputs):\n",
    "    return Forall(Implies(popularity_msd, Not(outputs['rec'])))\n",
    "novelty_evalution(utils.common.msd, 'MSD', msd_novelty_constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 Loss at step 2457: 0.2415, time: 334.312. Train accuracy 0.817, Validation accuracy 0.732\n",
      "Epoch #0 Loss at step 2457: 0.2650, time: 337.237. Train accuracy 0.814, Validation accuracy 0.608\n",
      "Epoch #0 Loss at step 2457: 0.2491, time: 335.453. Train accuracy 0.828, Validation accuracy 0.711\n",
      "Epoch #0 Loss at step 2457: 0.2502, time: 335.168. Train accuracy 0.795, Validation accuracy 0.677\n",
      "Epoch #0 Loss at step 2457: 0.2477, time: 335.165. Train accuracy 0.808, Validation accuracy 0.698\n",
      "Epoch #0 Loss at step 2457: 0.2524, time: 336.234. Train accuracy 0.795, Validation accuracy 0.626\n",
      "Epoch #0 Loss at step 2457: 0.2340, time: 336.660. Train accuracy 0.817, Validation accuracy 0.604\n",
      "Epoch #0 Loss at step 2457: 0.2466, time: 335.393. Train accuracy 0.838, Validation accuracy 0.689\n",
      "Epoch #0 Loss at step 2457: 0.2288, time: 334.526. Train accuracy 0.837, Validation accuracy 0.726\n",
      "Epoch #0 Loss at step 2457: 0.2361, time: 335.579. Train accuracy 0.829, Validation accuracy 0.636\n",
      "Epoch #0 Loss at step 2457: 0.2481, time: 332.702. Train accuracy 0.832, Validation accuracy 0.650\n",
      "Epoch #0 Loss at step 2457: 0.2070, time: 336.373. Train accuracy 0.823, Validation accuracy 0.645\n",
      "Epoch #0 Loss at step 2457: 0.2525, time: 336.338. Train accuracy 0.832, Validation accuracy 0.645\n",
      "Epoch #0 Loss at step 2457: 0.2301, time: 336.063. Train accuracy 0.805, Validation accuracy 0.591\n",
      "Epoch #0 Loss at step 2457: 0.2301, time: 337.349. Train accuracy 0.826, Validation accuracy 0.654\n",
      "Epoch #0 Loss at step 2457: 0.2457, time: 334.313. Train accuracy 0.815, Validation accuracy 0.664\n",
      "Batch nr 186 predicted\r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 Loss at step 2885: 0.1795, time: 557.274. Train accuracy 0.861, Validation accuracy 0.799\n",
      "Epoch #0 Loss at step 2885: 0.2217, time: 548.487. Train accuracy 0.834, Validation accuracy 0.706\n",
      "Epoch #0 Loss at step 2885: 0.1775, time: 551.901. Train accuracy 0.866, Validation accuracy 0.760\n",
      "Epoch #0 Loss at step 2885: 0.2061, time: 546.200. Train accuracy 0.872, Validation accuracy 0.743\n",
      "Epoch #0 Loss at step 2885: 0.1933, time: 546.090. Train accuracy 0.841, Validation accuracy 0.625\n",
      "Epoch #0 Loss at step 2885: 0.1972, time: 545.309. Train accuracy 0.835, Validation accuracy 0.702\n",
      "Epoch #0 Loss at step 2885: 0.1890, time: 546.188. Train accuracy 0.829, Validation accuracy 0.690\n",
      "Epoch #0 Loss at step 2885: 0.2529, time: 546.056. Train accuracy 0.865, Validation accuracy 0.758\n",
      "Epoch #0 Loss at step 2885: 0.2130, time: 547.248. Train accuracy 0.836, Validation accuracy 0.627\n",
      "Epoch #0 Loss at step 2885: 0.1946, time: 546.237. Train accuracy 0.865, Validation accuracy 0.668\n",
      "Epoch #0 Loss at step 2885: 0.1963, time: 545.324. Train accuracy 0.846, Validation accuracy 0.715\n",
      "Epoch #0 Loss at step 2885: 0.1919, time: 545.975. Train accuracy 0.852, Validation accuracy 0.637\n",
      "Epoch #0 Loss at step 2885: 0.2244, time: 545.649. Train accuracy 0.826, Validation accuracy 0.665\n",
      "Epoch #0 Loss at step 2885: 0.1777, time: 546.952. Train accuracy 0.843, Validation accuracy 0.710\n",
      "Epoch #0 Loss at step 2885: 0.1789, time: 546.213. Train accuracy 0.821, Validation accuracy 0.663\n",
      "Epoch #0 Loss at step 2885: 0.1699, time: 547.087. Train accuracy 0.847, Validation accuracy 0.714\n",
      "Batch nr 214 predicted\r"
     ]
    }
   ],
   "source": [
    "novelty_evalution(utils.common.movie_lens, 'ML', ml_pretrain, 'NeuralLogicRec_pretrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1],\n",
       "       [26, 27],\n",
       "       [84, 85]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(100).reshape(50,2)[np.random.rand(50) > 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook NeuralLogicRecEvaluation.ipynb to html\n",
      "[NbConvertApp] Writing 345568 bytes to NeuralLogicRecEvaluation.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html NeuralLogicRecEvaluation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
