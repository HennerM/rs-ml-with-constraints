{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = list() \n",
    "ae_evals = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils.common)\n",
    "importlib.reload(evaluation)\n",
    "importlib.reload(models.NeuralLogicRec)\n",
    "import utils.common\n",
    "import evaluation\n",
    "import importlib\n",
    "import numpy as np\n",
    "import time\n",
    "from models.ConstraintAutoRec import ConstraintAutoRec \n",
    "import models.NeuralLogicRec\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from models.NeuralLogicRec import item_cf, user_cf, diversity_constraint, Constraint, And, Or, Implies, Forall, Not, Equiv\n",
    "\n",
    "\n",
    "import itertools\n",
    "\n",
    "ml_small = utils.common.ml_small\n",
    "ml_big = utils.common.movie_lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Constraint(weight=0.4, formula=<tensorflow.python.eager.def_function.Function object at 0x7fa85fce4390>),\n",
       " Constraint(weight=1.0, formula=<tensorflow.python.eager.def_function.Function object at 0x7fa8602283c8>),\n",
       " Constraint(weight=0.75, formula=<tensorflow.python.eager.def_function.Function object at 0x7fa860037160>),\n",
       " Constraint(weight=0.25, formula=<tensorflow.python.eager.def_function.Function object at 0x7fa85ff0f080>),\n",
       " Constraint(weight=0.25, formula=<tensorflow.python.eager.def_function.Function object at 0x7fa85fce4588>)]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints = list()\n",
    "constraints.append(Constraint(weight=0.4, formula=item_cf))\n",
    "constraints.append(Constraint(weight=1.0, formula=user_cf))\n",
    "@tf.function\n",
    "def likes_equiv(model, outputs):\n",
    "    return Forall(Equiv(outputs['rec'], outputs['likes']))\n",
    "constraints.append(Constraint(weight=0.75, formula=likes_equiv))\n",
    "@tf.function\n",
    "def novelty_constraint(model, outputs):\n",
    "    return Forall(Implies(outputs['popular'], Not(outputs['rec'])))\n",
    "constraints.append(Constraint(weight=0.25, formula=novelty_constraint))\n",
    "constraints.append(Constraint(weight=0.25, formula=diversity_constraint))\n",
    "constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ev = evaluation.Evaluation(ml_big)\n",
    "nlr = models.NeuralLogicRec.NLR(ml_big['user'], ml_big['dimensions'], epochs=1, embedding_dim=40, batch_size=48, nr_hidden_layers=3, nr_item_samples = 4096, constraints=constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:414: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 Loss at step 3: 0.5352, time: 12.649. Train accuracy 0.639, Validation accuracy 0.180\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:414: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 Loss at step 12: 0.4898, time: 20.923\n",
      "Epoch #1 Loss at step 12: 0.4192, time: 2.646 Train accuracy 0.673, Validation accuracy 0.214\n",
      "Epoch #2 Loss at step 12: 0.3684, time: 2.652 Train accuracy 0.634, Validation accuracy 0.128\n",
      "Epoch #3 Loss at step 12: 0.3868, time: 2.634 Train accuracy 0.648, Validation accuracy 0.125\n",
      "Epoch #4 Loss at step 12: 0.3130, time: 2.643 Train accuracy 0.653, Validation accuracy 0.157\n",
      "Batch nr 2 predicted\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 Loss at step 12: 0.2854, time: 2.651 Train accuracy 0.676, Validation accuracy 0.252\n",
      "Epoch #1 Loss at step 12: 0.2631, time: 2.629 Train accuracy 0.660, Validation accuracy 0.245\n",
      "Epoch #2 Loss at step 12: 0.2505, time: 2.634 Train accuracy 0.650, Validation accuracy 0.141\n",
      "Epoch #3 Loss at step 12: 0.2171, time: 2.640 Train accuracy 0.662, Validation accuracy 0.189\n",
      "Epoch #4 Loss at step 12: 0.2003, time: 2.647 Train accuracy 0.676, Validation accuracy 0.252\n",
      "Batch nr 2 predicted\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 Loss at step 12: 0.2059, time: 2.632 Train accuracy 0.659, Validation accuracy 0.165\n",
      "Epoch #1 Loss at step 12: 0.2006, time: 2.639 Train accuracy 0.650, Validation accuracy 0.153\n",
      "Epoch #2 Loss at step 12: 0.1818, time: 2.639 Train accuracy 0.650, Validation accuracy 0.153\n",
      "Epoch #3 Loss at step 12: 0.1714, time: 2.637 Train accuracy 0.677, Validation accuracy 0.240\n",
      "Epoch #4 Loss at step 12: 0.1570, time: 2.637 Train accuracy 0.707, Validation accuracy 0.298\n",
      "Batch nr 2 predicted\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 Loss at step 12: 0.1665, time: 2.652 Train accuracy 0.638, Validation accuracy 0.147\n",
      "Epoch #1 Loss at step 12: 0.1605, time: 2.646 Train accuracy 0.647, Validation accuracy 0.167\n",
      "Epoch #2 Loss at step 12: 0.1786, time: 2.653 Train accuracy 0.668, Validation accuracy 0.211\n",
      "Epoch #3 Loss at step 12: 0.1405, time: 2.642 Train accuracy 0.668, Validation accuracy 0.211\n",
      "Epoch #4 Loss at step 12: 0.1320, time: 2.636 Train accuracy 0.647, Validation accuracy 0.167\n",
      "Epoch #0 Loss at step 12: 0.1395, time: 2.626 Train accuracy 0.693, Validation accuracy 0.273\n",
      "Epoch #1 Loss at step 12: 0.1246, time: 2.625 Train accuracy 0.637, Validation accuracy 0.155\n",
      "Epoch #2 Loss at step 12: 0.1237, time: 2.639 Train accuracy 0.653, Validation accuracy 0.142\n",
      "Epoch #3 Loss at step 12: 0.1273, time: 2.633 Train accuracy 0.665, Validation accuracy 0.218\n",
      "Epoch #4 Loss at step 12: 0.1109, time: 2.618 Train accuracy 0.671, Validation accuracy 0.216\n",
      "Epoch #0 Loss at step 12: 0.1471, time: 2.644 Train accuracy 0.701, Validation accuracy 0.267\n",
      "Epoch #1 Loss at step 12: 0.1128, time: 2.641 Train accuracy 0.753, Validation accuracy 0.391\n",
      "Epoch #2 Loss at step 12: 0.1066, time: 2.644 Train accuracy 0.749, Validation accuracy 0.426\n",
      "Epoch #3 Loss at step 12: 0.0991, time: 2.646 Train accuracy 0.738, Validation accuracy 0.413\n",
      "Epoch #4 Loss at step 12: 0.0929, time: 2.646 Train accuracy 0.727, Validation accuracy 0.376\n",
      "Epoch #0 Loss at step 12: 0.0980, time: 2.624 Train accuracy 0.707, Validation accuracy 0.292\n",
      "Epoch #1 Loss at step 12: 0.0924, time: 2.635 Train accuracy 0.750, Validation accuracy 0.447\n",
      "Epoch #2 Loss at step 12: 0.1022, time: 2.643 Train accuracy 0.754, Validation accuracy 0.426\n",
      "Epoch #3 Loss at step 12: 0.0949, time: 2.630 Train accuracy 0.774, Validation accuracy 0.546\n",
      "Epoch #4 Loss at step 12: 0.1070, time: 2.642 Train accuracy 0.757, Validation accuracy 0.453\n",
      "Epoch #0 Loss at step 12: 0.0845, time: 2.633 Train accuracy 0.766, Validation accuracy 0.484\n",
      "Epoch #1 Loss at step 12: 0.1005, time: 2.644 Train accuracy 0.770, Validation accuracy 0.530\n",
      "Epoch #2 Loss at step 12: 0.0890, time: 2.636 Train accuracy 0.769, Validation accuracy 0.459\n",
      "Epoch #3 Loss at step 12: 0.0780, time: 2.641 Train accuracy 0.725, Validation accuracy 0.329\n",
      "Epoch #4 Loss at step 12: 0.0800, time: 2.643 Train accuracy 0.747, Validation accuracy 0.401\n",
      "Batch nr 2 predicted\r"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    nlr.train(utils.common.load_dataset(ml_big), ml_big['train']['records'])\n",
    "    ae_evals.append(ev.evaluate_single_thread(nlr, max_nr_batches=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>coverage@1</th>\n",
       "      <th>coverage@10</th>\n",
       "      <th>coverage@5</th>\n",
       "      <th>diversity@10</th>\n",
       "      <th>diversity@5</th>\n",
       "      <th>embedding_dim</th>\n",
       "      <th>epc@10</th>\n",
       "      <th>epc@5</th>\n",
       "      <th>epd@5</th>\n",
       "      <th>epochs_trained</th>\n",
       "      <th>map@1</th>\n",
       "      <th>map@10</th>\n",
       "      <th>map@5</th>\n",
       "      <th>name</th>\n",
       "      <th>nr_hidden_layers</th>\n",
       "      <th>nr_item_samples</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.007225</td>\n",
       "      <td>0.038917</td>\n",
       "      <td>0.023408</td>\n",
       "      <td>0.383350</td>\n",
       "      <td>0.387613</td>\n",
       "      <td>48</td>\n",
       "      <td>0.923160</td>\n",
       "      <td>0.917086</td>\n",
       "      <td>0.369262</td>\n",
       "      <td>10</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.004322</td>\n",
       "      <td>0.005991</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.004720</td>\n",
       "      <td>0.026105</td>\n",
       "      <td>0.014931</td>\n",
       "      <td>0.353366</td>\n",
       "      <td>0.350754</td>\n",
       "      <td>48</td>\n",
       "      <td>0.845266</td>\n",
       "      <td>0.839974</td>\n",
       "      <td>0.352167</td>\n",
       "      <td>15</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.011153</td>\n",
       "      <td>0.016682</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.090550</td>\n",
       "      <td>0.054715</td>\n",
       "      <td>0.348376</td>\n",
       "      <td>0.342090</td>\n",
       "      <td>48</td>\n",
       "      <td>0.897102</td>\n",
       "      <td>0.897187</td>\n",
       "      <td>0.352894</td>\n",
       "      <td>20</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.006702</td>\n",
       "      <td>0.008889</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.010211</td>\n",
       "      <td>0.062133</td>\n",
       "      <td>0.039495</td>\n",
       "      <td>0.331413</td>\n",
       "      <td>0.326590</td>\n",
       "      <td>48</td>\n",
       "      <td>0.829359</td>\n",
       "      <td>0.821997</td>\n",
       "      <td>0.345213</td>\n",
       "      <td>25</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.016593</td>\n",
       "      <td>0.025556</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.010885</td>\n",
       "      <td>0.007032</td>\n",
       "      <td>0.376349</td>\n",
       "      <td>0.377160</td>\n",
       "      <td>48</td>\n",
       "      <td>0.965051</td>\n",
       "      <td>0.966749</td>\n",
       "      <td>0.375485</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.011656</td>\n",
       "      <td>0.007995</td>\n",
       "      <td>0.316753</td>\n",
       "      <td>0.297277</td>\n",
       "      <td>48</td>\n",
       "      <td>0.771557</td>\n",
       "      <td>0.751873</td>\n",
       "      <td>0.333980</td>\n",
       "      <td>10</td>\n",
       "      <td>0.076577</td>\n",
       "      <td>0.023973</td>\n",
       "      <td>0.032538</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.016280</td>\n",
       "      <td>0.011849</td>\n",
       "      <td>0.310953</td>\n",
       "      <td>0.297450</td>\n",
       "      <td>48</td>\n",
       "      <td>0.740281</td>\n",
       "      <td>0.732255</td>\n",
       "      <td>0.333925</td>\n",
       "      <td>15</td>\n",
       "      <td>0.112613</td>\n",
       "      <td>0.040542</td>\n",
       "      <td>0.054444</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.279820</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.008381</td>\n",
       "      <td>0.004816</td>\n",
       "      <td>0.291057</td>\n",
       "      <td>0.302842</td>\n",
       "      <td>48</td>\n",
       "      <td>0.713602</td>\n",
       "      <td>0.707426</td>\n",
       "      <td>0.339348</td>\n",
       "      <td>20</td>\n",
       "      <td>0.090090</td>\n",
       "      <td>0.035919</td>\n",
       "      <td>0.048949</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.940640</td>\n",
       "      <td>0.166344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005394</td>\n",
       "      <td>0.034004</td>\n",
       "      <td>0.027839</td>\n",
       "      <td>0.306129</td>\n",
       "      <td>0.304598</td>\n",
       "      <td>48</td>\n",
       "      <td>0.759864</td>\n",
       "      <td>0.744240</td>\n",
       "      <td>0.340192</td>\n",
       "      <td>25</td>\n",
       "      <td>0.067568</td>\n",
       "      <td>0.027048</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.010307</td>\n",
       "      <td>0.006165</td>\n",
       "      <td>0.364703</td>\n",
       "      <td>0.361235</td>\n",
       "      <td>48</td>\n",
       "      <td>0.950213</td>\n",
       "      <td>0.945827</td>\n",
       "      <td>0.357151</td>\n",
       "      <td>5</td>\n",
       "      <td>0.022523</td>\n",
       "      <td>0.005402</td>\n",
       "      <td>0.008221</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002794</td>\n",
       "      <td>0.011367</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.337824</td>\n",
       "      <td>0.351743</td>\n",
       "      <td>48</td>\n",
       "      <td>0.772071</td>\n",
       "      <td>0.762095</td>\n",
       "      <td>0.347171</td>\n",
       "      <td>10</td>\n",
       "      <td>0.022523</td>\n",
       "      <td>0.017880</td>\n",
       "      <td>0.021708</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.004913</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>0.013775</td>\n",
       "      <td>0.317837</td>\n",
       "      <td>0.307619</td>\n",
       "      <td>48</td>\n",
       "      <td>0.724748</td>\n",
       "      <td>0.715139</td>\n",
       "      <td>0.339163</td>\n",
       "      <td>15</td>\n",
       "      <td>0.090090</td>\n",
       "      <td>0.041055</td>\n",
       "      <td>0.049565</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.450137</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.003275</td>\n",
       "      <td>0.002408</td>\n",
       "      <td>0.311393</td>\n",
       "      <td>0.283570</td>\n",
       "      <td>48</td>\n",
       "      <td>0.633834</td>\n",
       "      <td>0.608385</td>\n",
       "      <td>0.332339</td>\n",
       "      <td>20</td>\n",
       "      <td>0.148649</td>\n",
       "      <td>0.057552</td>\n",
       "      <td>0.071769</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.919012</td>\n",
       "      <td>0.394483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.426407</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.003372</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.290632</td>\n",
       "      <td>0.245236</td>\n",
       "      <td>48</td>\n",
       "      <td>0.612325</td>\n",
       "      <td>0.590195</td>\n",
       "      <td>0.320156</td>\n",
       "      <td>25</td>\n",
       "      <td>0.193694</td>\n",
       "      <td>0.070100</td>\n",
       "      <td>0.092237</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.913462</td>\n",
       "      <td>0.360523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.018592</td>\n",
       "      <td>0.010211</td>\n",
       "      <td>0.386011</td>\n",
       "      <td>0.389257</td>\n",
       "      <td>48</td>\n",
       "      <td>0.964432</td>\n",
       "      <td>0.968110</td>\n",
       "      <td>0.377706</td>\n",
       "      <td>5</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002890</td>\n",
       "      <td>0.016183</td>\n",
       "      <td>0.010307</td>\n",
       "      <td>0.340473</td>\n",
       "      <td>0.322223</td>\n",
       "      <td>48</td>\n",
       "      <td>0.771070</td>\n",
       "      <td>0.772885</td>\n",
       "      <td>0.342893</td>\n",
       "      <td>10</td>\n",
       "      <td>0.045045</td>\n",
       "      <td>0.020523</td>\n",
       "      <td>0.027357</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.003468</td>\n",
       "      <td>0.035257</td>\n",
       "      <td>0.025046</td>\n",
       "      <td>0.318245</td>\n",
       "      <td>0.313378</td>\n",
       "      <td>48</td>\n",
       "      <td>0.743086</td>\n",
       "      <td>0.721581</td>\n",
       "      <td>0.340703</td>\n",
       "      <td>15</td>\n",
       "      <td>0.063063</td>\n",
       "      <td>0.034302</td>\n",
       "      <td>0.048318</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005683</td>\n",
       "      <td>0.039014</td>\n",
       "      <td>0.031885</td>\n",
       "      <td>0.326164</td>\n",
       "      <td>0.322552</td>\n",
       "      <td>48</td>\n",
       "      <td>0.778558</td>\n",
       "      <td>0.761102</td>\n",
       "      <td>0.344492</td>\n",
       "      <td>20</td>\n",
       "      <td>0.103604</td>\n",
       "      <td>0.030667</td>\n",
       "      <td>0.045015</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.009633</td>\n",
       "      <td>0.005587</td>\n",
       "      <td>0.398929</td>\n",
       "      <td>0.396939</td>\n",
       "      <td>48</td>\n",
       "      <td>0.900612</td>\n",
       "      <td>0.881790</td>\n",
       "      <td>0.372777</td>\n",
       "      <td>5</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.009777</td>\n",
       "      <td>0.014444</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002505</td>\n",
       "      <td>0.010789</td>\n",
       "      <td>0.006261</td>\n",
       "      <td>0.324409</td>\n",
       "      <td>0.295736</td>\n",
       "      <td>48</td>\n",
       "      <td>0.747664</td>\n",
       "      <td>0.741873</td>\n",
       "      <td>0.334724</td>\n",
       "      <td>10</td>\n",
       "      <td>0.099099</td>\n",
       "      <td>0.032406</td>\n",
       "      <td>0.041907</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005105</td>\n",
       "      <td>0.019266</td>\n",
       "      <td>0.013101</td>\n",
       "      <td>0.309983</td>\n",
       "      <td>0.297937</td>\n",
       "      <td>48</td>\n",
       "      <td>0.736271</td>\n",
       "      <td>0.724980</td>\n",
       "      <td>0.339368</td>\n",
       "      <td>15</td>\n",
       "      <td>0.112613</td>\n",
       "      <td>0.043413</td>\n",
       "      <td>0.061471</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005202</td>\n",
       "      <td>0.024564</td>\n",
       "      <td>0.018784</td>\n",
       "      <td>0.296588</td>\n",
       "      <td>0.291519</td>\n",
       "      <td>48</td>\n",
       "      <td>0.723956</td>\n",
       "      <td>0.710647</td>\n",
       "      <td>0.336665</td>\n",
       "      <td>20</td>\n",
       "      <td>0.090090</td>\n",
       "      <td>0.035508</td>\n",
       "      <td>0.047237</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.003468</td>\n",
       "      <td>0.016183</td>\n",
       "      <td>0.010115</td>\n",
       "      <td>0.383841</td>\n",
       "      <td>0.381664</td>\n",
       "      <td>48</td>\n",
       "      <td>0.943809</td>\n",
       "      <td>0.947558</td>\n",
       "      <td>0.372739</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002435</td>\n",
       "      <td>0.001156</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>0.025431</td>\n",
       "      <td>0.014642</td>\n",
       "      <td>0.325092</td>\n",
       "      <td>0.316446</td>\n",
       "      <td>48</td>\n",
       "      <td>0.783757</td>\n",
       "      <td>0.782554</td>\n",
       "      <td>0.336793</td>\n",
       "      <td>10</td>\n",
       "      <td>0.045045</td>\n",
       "      <td>0.032863</td>\n",
       "      <td>0.040375</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005202</td>\n",
       "      <td>0.045468</td>\n",
       "      <td>0.030729</td>\n",
       "      <td>0.321350</td>\n",
       "      <td>0.315305</td>\n",
       "      <td>48</td>\n",
       "      <td>0.784342</td>\n",
       "      <td>0.766453</td>\n",
       "      <td>0.342600</td>\n",
       "      <td>15</td>\n",
       "      <td>0.112613</td>\n",
       "      <td>0.036992</td>\n",
       "      <td>0.052342</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.552686</td>\n",
       "      <td>32</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.009633</td>\n",
       "      <td>0.005298</td>\n",
       "      <td>0.266205</td>\n",
       "      <td>0.257546</td>\n",
       "      <td>48</td>\n",
       "      <td>0.621092</td>\n",
       "      <td>0.597097</td>\n",
       "      <td>0.321293</td>\n",
       "      <td>20</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.060499</td>\n",
       "      <td>0.075748</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.906659</td>\n",
       "      <td>0.531894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.009922</td>\n",
       "      <td>0.005587</td>\n",
       "      <td>0.361197</td>\n",
       "      <td>0.349796</td>\n",
       "      <td>48</td>\n",
       "      <td>0.967434</td>\n",
       "      <td>0.965205</td>\n",
       "      <td>0.359275</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.003468</td>\n",
       "      <td>0.016858</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.334101</td>\n",
       "      <td>0.349713</td>\n",
       "      <td>48</td>\n",
       "      <td>0.827767</td>\n",
       "      <td>0.821204</td>\n",
       "      <td>0.348916</td>\n",
       "      <td>10</td>\n",
       "      <td>0.063063</td>\n",
       "      <td>0.014976</td>\n",
       "      <td>0.019895</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>0.005683</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>0.308565</td>\n",
       "      <td>0.312831</td>\n",
       "      <td>48</td>\n",
       "      <td>0.677993</td>\n",
       "      <td>0.673651</td>\n",
       "      <td>0.343558</td>\n",
       "      <td>15</td>\n",
       "      <td>0.099099</td>\n",
       "      <td>0.046533</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.483439</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002890</td>\n",
       "      <td>0.018977</td>\n",
       "      <td>0.010789</td>\n",
       "      <td>0.284824</td>\n",
       "      <td>0.256566</td>\n",
       "      <td>48</td>\n",
       "      <td>0.652153</td>\n",
       "      <td>0.626759</td>\n",
       "      <td>0.323874</td>\n",
       "      <td>20</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.057262</td>\n",
       "      <td>0.075808</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.908678</td>\n",
       "      <td>0.449970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.599612</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005683</td>\n",
       "      <td>0.032078</td>\n",
       "      <td>0.020133</td>\n",
       "      <td>0.361099</td>\n",
       "      <td>0.367981</td>\n",
       "      <td>40</td>\n",
       "      <td>0.930763</td>\n",
       "      <td>0.933952</td>\n",
       "      <td>0.362294</td>\n",
       "      <td>35</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.006909</td>\n",
       "      <td>0.009189</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.913619</td>\n",
       "      <td>0.591401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.578954</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005202</td>\n",
       "      <td>0.026394</td>\n",
       "      <td>0.016665</td>\n",
       "      <td>0.359227</td>\n",
       "      <td>0.357126</td>\n",
       "      <td>40</td>\n",
       "      <td>0.904201</td>\n",
       "      <td>0.904929</td>\n",
       "      <td>0.358853</td>\n",
       "      <td>40</td>\n",
       "      <td>0.022523</td>\n",
       "      <td>0.008684</td>\n",
       "      <td>0.010696</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.918388</td>\n",
       "      <td>0.547955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.575199</td>\n",
       "      <td>32</td>\n",
       "      <td>0.003468</td>\n",
       "      <td>0.018784</td>\n",
       "      <td>0.012716</td>\n",
       "      <td>0.329151</td>\n",
       "      <td>0.336397</td>\n",
       "      <td>40</td>\n",
       "      <td>0.838656</td>\n",
       "      <td>0.839213</td>\n",
       "      <td>0.347328</td>\n",
       "      <td>46</td>\n",
       "      <td>0.049550</td>\n",
       "      <td>0.021451</td>\n",
       "      <td>0.027538</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.913823</td>\n",
       "      <td>0.548570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.577386</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.007610</td>\n",
       "      <td>0.272868</td>\n",
       "      <td>0.250170</td>\n",
       "      <td>40</td>\n",
       "      <td>0.743124</td>\n",
       "      <td>0.731053</td>\n",
       "      <td>0.320008</td>\n",
       "      <td>51</td>\n",
       "      <td>0.085586</td>\n",
       "      <td>0.039283</td>\n",
       "      <td>0.049082</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.918090</td>\n",
       "      <td>0.547848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.570294</td>\n",
       "      <td>32</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.010115</td>\n",
       "      <td>0.005780</td>\n",
       "      <td>0.258737</td>\n",
       "      <td>0.267161</td>\n",
       "      <td>40</td>\n",
       "      <td>0.697314</td>\n",
       "      <td>0.685440</td>\n",
       "      <td>0.327455</td>\n",
       "      <td>56</td>\n",
       "      <td>0.130631</td>\n",
       "      <td>0.051684</td>\n",
       "      <td>0.064189</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.915649</td>\n",
       "      <td>0.539268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.565490</td>\n",
       "      <td>32</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0.009922</td>\n",
       "      <td>0.005876</td>\n",
       "      <td>0.267309</td>\n",
       "      <td>0.255915</td>\n",
       "      <td>40</td>\n",
       "      <td>0.676242</td>\n",
       "      <td>0.628459</td>\n",
       "      <td>0.319716</td>\n",
       "      <td>61</td>\n",
       "      <td>0.180180</td>\n",
       "      <td>0.061875</td>\n",
       "      <td>0.091652</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.922412</td>\n",
       "      <td>0.526786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.592830</td>\n",
       "      <td>32</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.004239</td>\n",
       "      <td>0.257369</td>\n",
       "      <td>0.265140</td>\n",
       "      <td>40</td>\n",
       "      <td>0.753632</td>\n",
       "      <td>0.745188</td>\n",
       "      <td>0.321521</td>\n",
       "      <td>66</td>\n",
       "      <td>0.103604</td>\n",
       "      <td>0.041138</td>\n",
       "      <td>0.051403</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.906270</td>\n",
       "      <td>0.575683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.568475</td>\n",
       "      <td>32</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.007514</td>\n",
       "      <td>0.004335</td>\n",
       "      <td>0.283567</td>\n",
       "      <td>0.272181</td>\n",
       "      <td>40</td>\n",
       "      <td>0.742987</td>\n",
       "      <td>0.727802</td>\n",
       "      <td>0.327358</td>\n",
       "      <td>71</td>\n",
       "      <td>0.126126</td>\n",
       "      <td>0.040074</td>\n",
       "      <td>0.051306</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.926743</td>\n",
       "      <td>0.531698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.582741</td>\n",
       "      <td>32</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.007321</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>0.299750</td>\n",
       "      <td>0.251211</td>\n",
       "      <td>40</td>\n",
       "      <td>0.747978</td>\n",
       "      <td>0.708252</td>\n",
       "      <td>0.322483</td>\n",
       "      <td>76</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.045282</td>\n",
       "      <td>0.063630</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.917149</td>\n",
       "      <td>0.557269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.572680</td>\n",
       "      <td>32</td>\n",
       "      <td>0.001734</td>\n",
       "      <td>0.007899</td>\n",
       "      <td>0.005009</td>\n",
       "      <td>0.287248</td>\n",
       "      <td>0.280651</td>\n",
       "      <td>40</td>\n",
       "      <td>0.785818</td>\n",
       "      <td>0.775823</td>\n",
       "      <td>0.325920</td>\n",
       "      <td>81</td>\n",
       "      <td>0.045045</td>\n",
       "      <td>0.026991</td>\n",
       "      <td>0.032080</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.914156</td>\n",
       "      <td>0.544806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.583126</td>\n",
       "      <td>32</td>\n",
       "      <td>0.001638</td>\n",
       "      <td>0.007225</td>\n",
       "      <td>0.004528</td>\n",
       "      <td>0.282797</td>\n",
       "      <td>0.291435</td>\n",
       "      <td>40</td>\n",
       "      <td>0.798789</td>\n",
       "      <td>0.808362</td>\n",
       "      <td>0.328390</td>\n",
       "      <td>86</td>\n",
       "      <td>0.072072</td>\n",
       "      <td>0.031266</td>\n",
       "      <td>0.041769</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.914632</td>\n",
       "      <td>0.553864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.577229</td>\n",
       "      <td>32</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.006936</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>0.296619</td>\n",
       "      <td>0.311241</td>\n",
       "      <td>40</td>\n",
       "      <td>0.790849</td>\n",
       "      <td>0.804054</td>\n",
       "      <td>0.336867</td>\n",
       "      <td>91</td>\n",
       "      <td>0.063063</td>\n",
       "      <td>0.029335</td>\n",
       "      <td>0.040050</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.919793</td>\n",
       "      <td>0.549394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.582833</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>0.007225</td>\n",
       "      <td>0.004335</td>\n",
       "      <td>0.284405</td>\n",
       "      <td>0.298143</td>\n",
       "      <td>40</td>\n",
       "      <td>0.786278</td>\n",
       "      <td>0.791429</td>\n",
       "      <td>0.332098</td>\n",
       "      <td>96</td>\n",
       "      <td>0.094595</td>\n",
       "      <td>0.031394</td>\n",
       "      <td>0.043023</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.915871</td>\n",
       "      <td>0.560778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.575936</td>\n",
       "      <td>32</td>\n",
       "      <td>0.001638</td>\n",
       "      <td>0.008284</td>\n",
       "      <td>0.005009</td>\n",
       "      <td>0.279162</td>\n",
       "      <td>0.294220</td>\n",
       "      <td>40</td>\n",
       "      <td>0.826505</td>\n",
       "      <td>0.835713</td>\n",
       "      <td>0.323419</td>\n",
       "      <td>101</td>\n",
       "      <td>0.076577</td>\n",
       "      <td>0.026266</td>\n",
       "      <td>0.039399</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.916886</td>\n",
       "      <td>0.552758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002794</td>\n",
       "      <td>0.012427</td>\n",
       "      <td>0.007803</td>\n",
       "      <td>0.364770</td>\n",
       "      <td>0.381082</td>\n",
       "      <td>40</td>\n",
       "      <td>0.938844</td>\n",
       "      <td>0.929089</td>\n",
       "      <td>0.366778</td>\n",
       "      <td>5</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.007604</td>\n",
       "      <td>0.012267</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>32</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>0.005876</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>0.322228</td>\n",
       "      <td>0.339498</td>\n",
       "      <td>40</td>\n",
       "      <td>0.793295</td>\n",
       "      <td>0.794353</td>\n",
       "      <td>0.344361</td>\n",
       "      <td>10</td>\n",
       "      <td>0.063063</td>\n",
       "      <td>0.021215</td>\n",
       "      <td>0.026306</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.148195</td>\n",
       "      <td>32</td>\n",
       "      <td>0.005972</td>\n",
       "      <td>0.022638</td>\n",
       "      <td>0.017147</td>\n",
       "      <td>0.294884</td>\n",
       "      <td>0.280090</td>\n",
       "      <td>40</td>\n",
       "      <td>0.757331</td>\n",
       "      <td>0.748326</td>\n",
       "      <td>0.325606</td>\n",
       "      <td>15</td>\n",
       "      <td>0.099099</td>\n",
       "      <td>0.033056</td>\n",
       "      <td>0.042974</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.992278</td>\n",
       "      <td>0.003951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.382372</td>\n",
       "      <td>32</td>\n",
       "      <td>0.003275</td>\n",
       "      <td>0.014353</td>\n",
       "      <td>0.009055</td>\n",
       "      <td>0.285869</td>\n",
       "      <td>0.263852</td>\n",
       "      <td>40</td>\n",
       "      <td>0.697044</td>\n",
       "      <td>0.671402</td>\n",
       "      <td>0.318111</td>\n",
       "      <td>20</td>\n",
       "      <td>0.139640</td>\n",
       "      <td>0.049867</td>\n",
       "      <td>0.065320</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.921019</td>\n",
       "      <td>0.308441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.354455</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.015316</td>\n",
       "      <td>0.009729</td>\n",
       "      <td>0.284668</td>\n",
       "      <td>0.263779</td>\n",
       "      <td>40</td>\n",
       "      <td>0.648127</td>\n",
       "      <td>0.623073</td>\n",
       "      <td>0.322990</td>\n",
       "      <td>25</td>\n",
       "      <td>0.144144</td>\n",
       "      <td>0.049440</td>\n",
       "      <td>0.066847</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.908315</td>\n",
       "      <td>0.262898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.253694</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>0.002408</td>\n",
       "      <td>0.274996</td>\n",
       "      <td>0.247428</td>\n",
       "      <td>40</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.584828</td>\n",
       "      <td>0.319500</td>\n",
       "      <td>30</td>\n",
       "      <td>0.180180</td>\n",
       "      <td>0.063408</td>\n",
       "      <td>0.082853</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.957483</td>\n",
       "      <td>0.128667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.297323</td>\n",
       "      <td>32</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>0.008766</td>\n",
       "      <td>0.005202</td>\n",
       "      <td>0.282129</td>\n",
       "      <td>0.263365</td>\n",
       "      <td>40</td>\n",
       "      <td>0.625122</td>\n",
       "      <td>0.592721</td>\n",
       "      <td>0.322976</td>\n",
       "      <td>35</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.064428</td>\n",
       "      <td>0.088556</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.926901</td>\n",
       "      <td>0.187047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.353115</td>\n",
       "      <td>32</td>\n",
       "      <td>0.001156</td>\n",
       "      <td>0.005972</td>\n",
       "      <td>0.003275</td>\n",
       "      <td>0.275967</td>\n",
       "      <td>0.247404</td>\n",
       "      <td>40</td>\n",
       "      <td>0.612462</td>\n",
       "      <td>0.586839</td>\n",
       "      <td>0.318764</td>\n",
       "      <td>40</td>\n",
       "      <td>0.180180</td>\n",
       "      <td>0.066212</td>\n",
       "      <td>0.087838</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.928153</td>\n",
       "      <td>0.257070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>48</td>\n",
       "      <td>0.003468</td>\n",
       "      <td>0.017436</td>\n",
       "      <td>0.010693</td>\n",
       "      <td>0.410958</td>\n",
       "      <td>0.415321</td>\n",
       "      <td>40</td>\n",
       "      <td>0.983952</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.399161</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.001336</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>48</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.004816</td>\n",
       "      <td>0.003083</td>\n",
       "      <td>0.393815</td>\n",
       "      <td>0.391691</td>\n",
       "      <td>40</td>\n",
       "      <td>0.922231</td>\n",
       "      <td>0.919081</td>\n",
       "      <td>0.361108</td>\n",
       "      <td>10</td>\n",
       "      <td>0.018018</td>\n",
       "      <td>0.006137</td>\n",
       "      <td>0.009002</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.144880</td>\n",
       "      <td>48</td>\n",
       "      <td>0.004239</td>\n",
       "      <td>0.014835</td>\n",
       "      <td>0.010982</td>\n",
       "      <td>0.341984</td>\n",
       "      <td>0.330628</td>\n",
       "      <td>40</td>\n",
       "      <td>0.810770</td>\n",
       "      <td>0.811894</td>\n",
       "      <td>0.352542</td>\n",
       "      <td>15</td>\n",
       "      <td>0.036036</td>\n",
       "      <td>0.018829</td>\n",
       "      <td>0.023173</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.144974</td>\n",
       "      <td>48</td>\n",
       "      <td>0.008573</td>\n",
       "      <td>0.042000</td>\n",
       "      <td>0.031693</td>\n",
       "      <td>0.328899</td>\n",
       "      <td>0.314122</td>\n",
       "      <td>40</td>\n",
       "      <td>0.804036</td>\n",
       "      <td>0.794526</td>\n",
       "      <td>0.346818</td>\n",
       "      <td>20</td>\n",
       "      <td>0.072072</td>\n",
       "      <td>0.023245</td>\n",
       "      <td>0.032417</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.258817</td>\n",
       "      <td>48</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.023601</td>\n",
       "      <td>0.015605</td>\n",
       "      <td>0.331693</td>\n",
       "      <td>0.316916</td>\n",
       "      <td>40</td>\n",
       "      <td>0.793903</td>\n",
       "      <td>0.786108</td>\n",
       "      <td>0.347089</td>\n",
       "      <td>25</td>\n",
       "      <td>0.049550</td>\n",
       "      <td>0.023729</td>\n",
       "      <td>0.032718</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.936321</td>\n",
       "      <td>0.139600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.357247</td>\n",
       "      <td>48</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>0.013101</td>\n",
       "      <td>0.007995</td>\n",
       "      <td>0.314484</td>\n",
       "      <td>0.305901</td>\n",
       "      <td>40</td>\n",
       "      <td>0.708617</td>\n",
       "      <td>0.679893</td>\n",
       "      <td>0.337615</td>\n",
       "      <td>30</td>\n",
       "      <td>0.121622</td>\n",
       "      <td>0.037745</td>\n",
       "      <td>0.053303</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.928513</td>\n",
       "      <td>0.257132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.425358</td>\n",
       "      <td>48</td>\n",
       "      <td>0.003275</td>\n",
       "      <td>0.018495</td>\n",
       "      <td>0.011752</td>\n",
       "      <td>0.295603</td>\n",
       "      <td>0.281926</td>\n",
       "      <td>40</td>\n",
       "      <td>0.685495</td>\n",
       "      <td>0.659094</td>\n",
       "      <td>0.330498</td>\n",
       "      <td>35</td>\n",
       "      <td>0.153153</td>\n",
       "      <td>0.050947</td>\n",
       "      <td>0.072027</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.926946</td>\n",
       "      <td>0.347996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.226962</td>\n",
       "      <td>48</td>\n",
       "      <td>0.003372</td>\n",
       "      <td>0.018014</td>\n",
       "      <td>0.010885</td>\n",
       "      <td>0.299117</td>\n",
       "      <td>0.284096</td>\n",
       "      <td>40</td>\n",
       "      <td>0.688749</td>\n",
       "      <td>0.668335</td>\n",
       "      <td>0.332124</td>\n",
       "      <td>40</td>\n",
       "      <td>0.112613</td>\n",
       "      <td>0.049895</td>\n",
       "      <td>0.061667</td>\n",
       "      <td>NeuralLogicRec</td>\n",
       "      <td>3</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.930746</td>\n",
       "      <td>0.098891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows  20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  batch_size  coverage@1  coverage@10  coverage@5  diversity@10  \\\n",
       "0   0.144880          32    0.007225     0.038917    0.023408      0.383350   \n",
       "1   0.144880          32    0.004720     0.026105    0.014931      0.353366   \n",
       "2   0.144880          32    0.014064     0.090550    0.054715      0.348376   \n",
       "3   0.144880          32    0.010211     0.062133    0.039495      0.331413   \n",
       "4   0.144880          32    0.002216     0.010885    0.007032      0.376349   \n",
       "5   0.144880          32    0.002312     0.011656    0.007995      0.316753   \n",
       "6   0.144880          32    0.003950     0.016280    0.011849      0.310953   \n",
       "7   0.279820          32    0.002119     0.008381    0.004816      0.291057   \n",
       "8   0.144880          32    0.005394     0.034004    0.027839      0.306129   \n",
       "9   0.144880          32    0.001541     0.010307    0.006165      0.364703   \n",
       "10  0.144880          32    0.002794     0.011367    0.007706      0.337824   \n",
       "11  0.144880          32    0.004913     0.021000    0.013775      0.317837   \n",
       "12  0.450137          32    0.000771     0.003275    0.002408      0.311393   \n",
       "13  0.426407          32    0.000963     0.003372    0.002312      0.290632   \n",
       "14  0.144880          32    0.003564     0.018592    0.010211      0.386011   \n",
       "15  0.144880          32    0.002890     0.016183    0.010307      0.340473   \n",
       "16  0.144880          32    0.003468     0.035257    0.025046      0.318245   \n",
       "17  0.144880          32    0.005683     0.039014    0.031885      0.326164   \n",
       "18  0.144880          32    0.002119     0.009633    0.005587      0.398929   \n",
       "19  0.144880          32    0.002505     0.010789    0.006261      0.324409   \n",
       "20  0.144880          32    0.005105     0.019266    0.013101      0.309983   \n",
       "21  0.144880          32    0.005202     0.024564    0.018784      0.296588   \n",
       "22  0.144880          32    0.003468     0.016183    0.010115      0.383841   \n",
       "23  0.144880          32    0.003179     0.025431    0.014642      0.325092   \n",
       "24  0.144880          32    0.005202     0.045468    0.030729      0.321350   \n",
       "25  0.552686          32    0.001541     0.009633    0.005298      0.266205   \n",
       "26  0.144880          32    0.002216     0.009922    0.005587      0.361197   \n",
       "27  0.144880          32    0.003468     0.016858    0.010500      0.334101   \n",
       "28  0.144880          32    0.002023     0.005683    0.004046      0.308565   \n",
       "29  0.483439          32    0.002890     0.018977    0.010789      0.284824   \n",
       "..       ...         ...         ...          ...         ...           ...   \n",
       "68  0.599612          32    0.005683     0.032078    0.020133      0.361099   \n",
       "69  0.578954          32    0.005202     0.026394    0.016665      0.359227   \n",
       "70  0.575199          32    0.003468     0.018784    0.012716      0.329151   \n",
       "71  0.577386          32    0.002312     0.014064    0.007610      0.272868   \n",
       "72  0.570294          32    0.001541     0.010115    0.005780      0.258737   \n",
       "73  0.565490          32    0.001445     0.009922    0.005876      0.267309   \n",
       "74  0.592830          32    0.001349     0.007706    0.004239      0.257369   \n",
       "75  0.568475          32    0.001349     0.007514    0.004335      0.283567   \n",
       "76  0.582741          32    0.001349     0.007321    0.004046      0.299750   \n",
       "77  0.572680          32    0.001734     0.007899    0.005009      0.287248   \n",
       "78  0.583126          32    0.001638     0.007225    0.004528      0.282797   \n",
       "79  0.577229          32    0.001349     0.006936    0.004142      0.296619   \n",
       "80  0.582833          32    0.002023     0.007225    0.004335      0.284405   \n",
       "81  0.575936          32    0.001638     0.008284    0.005009      0.279162   \n",
       "82  0.144880          32    0.002794     0.012427    0.007803      0.364770   \n",
       "83  0.144880          32    0.001830     0.005876    0.004142      0.322228   \n",
       "84  0.148195          32    0.005972     0.022638    0.017147      0.294884   \n",
       "85  0.382372          32    0.003275     0.014353    0.009055      0.285869   \n",
       "86  0.354455          32    0.002216     0.015316    0.009729      0.284668   \n",
       "87  0.253694          32    0.000867     0.003661    0.002408      0.274996   \n",
       "88  0.297323          32    0.001830     0.008766    0.005202      0.282129   \n",
       "89  0.353115          32    0.001156     0.005972    0.003275      0.275967   \n",
       "90  0.144880          48    0.003468     0.017436    0.010693      0.410958   \n",
       "91  0.144880          48    0.000963     0.004816    0.003083      0.393815   \n",
       "92  0.144880          48    0.004239     0.014835    0.010982      0.341984   \n",
       "93  0.144974          48    0.008573     0.042000    0.031693      0.328899   \n",
       "94  0.258817          48    0.003950     0.023601    0.015605      0.331693   \n",
       "95  0.357247          48    0.002697     0.013101    0.007995      0.314484   \n",
       "96  0.425358          48    0.003275     0.018495    0.011752      0.295603   \n",
       "97  0.226962          48    0.003372     0.018014    0.010885      0.299117   \n",
       "\n",
       "    diversity@5  embedding_dim    epc@10     epc@5     epd@5  epochs_trained  \\\n",
       "0      0.387613             48  0.923160  0.917086  0.369262              10   \n",
       "1      0.350754             48  0.845266  0.839974  0.352167              15   \n",
       "2      0.342090             48  0.897102  0.897187  0.352894              20   \n",
       "3      0.326590             48  0.829359  0.821997  0.345213              25   \n",
       "4      0.377160             48  0.965051  0.966749  0.375485               5   \n",
       "5      0.297277             48  0.771557  0.751873  0.333980              10   \n",
       "6      0.297450             48  0.740281  0.732255  0.333925              15   \n",
       "7      0.302842             48  0.713602  0.707426  0.339348              20   \n",
       "8      0.304598             48  0.759864  0.744240  0.340192              25   \n",
       "9      0.361235             48  0.950213  0.945827  0.357151               5   \n",
       "10     0.351743             48  0.772071  0.762095  0.347171              10   \n",
       "11     0.307619             48  0.724748  0.715139  0.339163              15   \n",
       "12     0.283570             48  0.633834  0.608385  0.332339              20   \n",
       "13     0.245236             48  0.612325  0.590195  0.320156              25   \n",
       "14     0.389257             48  0.964432  0.968110  0.377706               5   \n",
       "15     0.322223             48  0.771070  0.772885  0.342893              10   \n",
       "16     0.313378             48  0.743086  0.721581  0.340703              15   \n",
       "17     0.322552             48  0.778558  0.761102  0.344492              20   \n",
       "18     0.396939             48  0.900612  0.881790  0.372777               5   \n",
       "19     0.295736             48  0.747664  0.741873  0.334724              10   \n",
       "20     0.297937             48  0.736271  0.724980  0.339368              15   \n",
       "21     0.291519             48  0.723956  0.710647  0.336665              20   \n",
       "22     0.381664             48  0.943809  0.947558  0.372739               5   \n",
       "23     0.316446             48  0.783757  0.782554  0.336793              10   \n",
       "24     0.315305             48  0.784342  0.766453  0.342600              15   \n",
       "25     0.257546             48  0.621092  0.597097  0.321293              20   \n",
       "26     0.349796             48  0.967434  0.965205  0.359275               5   \n",
       "27     0.349713             48  0.827767  0.821204  0.348916              10   \n",
       "28     0.312831             48  0.677993  0.673651  0.343558              15   \n",
       "29     0.256566             48  0.652153  0.626759  0.323874              20   \n",
       "..          ...            ...       ...       ...       ...             ...   \n",
       "68     0.367981             40  0.930763  0.933952  0.362294              35   \n",
       "69     0.357126             40  0.904201  0.904929  0.358853              40   \n",
       "70     0.336397             40  0.838656  0.839213  0.347328              46   \n",
       "71     0.250170             40  0.743124  0.731053  0.320008              51   \n",
       "72     0.267161             40  0.697314  0.685440  0.327455              56   \n",
       "73     0.255915             40  0.676242  0.628459  0.319716              61   \n",
       "74     0.265140             40  0.753632  0.745188  0.321521              66   \n",
       "75     0.272181             40  0.742987  0.727802  0.327358              71   \n",
       "76     0.251211             40  0.747978  0.708252  0.322483              76   \n",
       "77     0.280651             40  0.785818  0.775823  0.325920              81   \n",
       "78     0.291435             40  0.798789  0.808362  0.328390              86   \n",
       "79     0.311241             40  0.790849  0.804054  0.336867              91   \n",
       "80     0.298143             40  0.786278  0.791429  0.332098              96   \n",
       "81     0.294220             40  0.826505  0.835713  0.323419             101   \n",
       "82     0.381082             40  0.938844  0.929089  0.366778               5   \n",
       "83     0.339498             40  0.793295  0.794353  0.344361              10   \n",
       "84     0.280090             40  0.757331  0.748326  0.325606              15   \n",
       "85     0.263852             40  0.697044  0.671402  0.318111              20   \n",
       "86     0.263779             40  0.648127  0.623073  0.322990              25   \n",
       "87     0.247428             40  0.610000  0.584828  0.319500              30   \n",
       "88     0.263365             40  0.625122  0.592721  0.322976              35   \n",
       "89     0.247404             40  0.612462  0.586839  0.318764              40   \n",
       "90     0.415321             40  0.983952  0.986111  0.399161               5   \n",
       "91     0.391691             40  0.922231  0.919081  0.361108              10   \n",
       "92     0.330628             40  0.810770  0.811894  0.352542              15   \n",
       "93     0.314122             40  0.804036  0.794526  0.346818              20   \n",
       "94     0.316916             40  0.793903  0.786108  0.347089              25   \n",
       "95     0.305901             40  0.708617  0.679893  0.337615              30   \n",
       "96     0.281926             40  0.685495  0.659094  0.330498              35   \n",
       "97     0.284096             40  0.688749  0.668335  0.332124              40   \n",
       "\n",
       "       map@1    map@10     map@5            name  nr_hidden_layers  \\\n",
       "0   0.013514  0.004322  0.005991  NeuralLogicRec                 3   \n",
       "1   0.040541  0.011153  0.016682  NeuralLogicRec                 3   \n",
       "2   0.013514  0.006702  0.008889  NeuralLogicRec                 3   \n",
       "3   0.054054  0.016593  0.025556  NeuralLogicRec                 3   \n",
       "4   0.000000  0.000920  0.001036  NeuralLogicRec                 3   \n",
       "5   0.076577  0.023973  0.032538  NeuralLogicRec                 3   \n",
       "6   0.112613  0.040542  0.054444  NeuralLogicRec                 3   \n",
       "7   0.090090  0.035919  0.048949  NeuralLogicRec                 3   \n",
       "8   0.067568  0.027048  0.035000  NeuralLogicRec                 3   \n",
       "9   0.022523  0.005402  0.008221  NeuralLogicRec                 3   \n",
       "10  0.022523  0.017880  0.021708  NeuralLogicRec                 3   \n",
       "11  0.090090  0.041055  0.049565  NeuralLogicRec                 3   \n",
       "12  0.148649  0.057552  0.071769  NeuralLogicRec                 3   \n",
       "13  0.193694  0.070100  0.092237  NeuralLogicRec                 3   \n",
       "14  0.009009  0.001866  0.002703  NeuralLogicRec                 3   \n",
       "15  0.045045  0.020523  0.027357  NeuralLogicRec                 3   \n",
       "16  0.063063  0.034302  0.048318  NeuralLogicRec                 3   \n",
       "17  0.103604  0.030667  0.045015  NeuralLogicRec                 3   \n",
       "18  0.040541  0.009777  0.014444  NeuralLogicRec                 3   \n",
       "19  0.099099  0.032406  0.041907  NeuralLogicRec                 3   \n",
       "20  0.112613  0.043413  0.061471  NeuralLogicRec                 3   \n",
       "21  0.090090  0.035508  0.047237  NeuralLogicRec                 3   \n",
       "22  0.000000  0.002435  0.001156  NeuralLogicRec                 3   \n",
       "23  0.045045  0.032863  0.040375  NeuralLogicRec                 3   \n",
       "24  0.112613  0.036992  0.052342  NeuralLogicRec                 3   \n",
       "25  0.166667  0.060499  0.075748  NeuralLogicRec                 3   \n",
       "26  0.000000  0.001504  0.002162  NeuralLogicRec                 3   \n",
       "27  0.063063  0.014976  0.019895  NeuralLogicRec                 3   \n",
       "28  0.099099  0.046533  0.055556  NeuralLogicRec                 3   \n",
       "29  0.162162  0.057262  0.075808  NeuralLogicRec                 3   \n",
       "..       ...       ...       ...             ...               ...   \n",
       "68  0.013514  0.006909  0.009189  NeuralLogicRec                 3   \n",
       "69  0.022523  0.008684  0.010696  NeuralLogicRec                 3   \n",
       "70  0.049550  0.021451  0.027538  NeuralLogicRec                 3   \n",
       "71  0.085586  0.039283  0.049082  NeuralLogicRec                 3   \n",
       "72  0.130631  0.051684  0.064189  NeuralLogicRec                 3   \n",
       "73  0.180180  0.061875  0.091652  NeuralLogicRec                 3   \n",
       "74  0.103604  0.041138  0.051403  NeuralLogicRec                 3   \n",
       "75  0.126126  0.040074  0.051306  NeuralLogicRec                 3   \n",
       "76  0.166667  0.045282  0.063630  NeuralLogicRec                 3   \n",
       "77  0.045045  0.026991  0.032080  NeuralLogicRec                 3   \n",
       "78  0.072072  0.031266  0.041769  NeuralLogicRec                 3   \n",
       "79  0.063063  0.029335  0.040050  NeuralLogicRec                 3   \n",
       "80  0.094595  0.031394  0.043023  NeuralLogicRec                 3   \n",
       "81  0.076577  0.026266  0.039399  NeuralLogicRec                 3   \n",
       "82  0.027027  0.007604  0.012267  NeuralLogicRec                 3   \n",
       "83  0.063063  0.021215  0.026306  NeuralLogicRec                 3   \n",
       "84  0.099099  0.033056  0.042974  NeuralLogicRec                 3   \n",
       "85  0.139640  0.049867  0.065320  NeuralLogicRec                 3   \n",
       "86  0.144144  0.049440  0.066847  NeuralLogicRec                 3   \n",
       "87  0.180180  0.063408  0.082853  NeuralLogicRec                 3   \n",
       "88  0.189189  0.064428  0.088556  NeuralLogicRec                 3   \n",
       "89  0.180180  0.066212  0.087838  NeuralLogicRec                 3   \n",
       "90  0.000000  0.000902  0.001336  NeuralLogicRec                 3   \n",
       "91  0.018018  0.006137  0.009002  NeuralLogicRec                 3   \n",
       "92  0.036036  0.018829  0.023173  NeuralLogicRec                 3   \n",
       "93  0.072072  0.023245  0.032417  NeuralLogicRec                 3   \n",
       "94  0.049550  0.023729  0.032718  NeuralLogicRec                 3   \n",
       "95  0.121622  0.037745  0.053303  NeuralLogicRec                 3   \n",
       "96  0.153153  0.050947  0.072027  NeuralLogicRec                 3   \n",
       "97  0.112613  0.049895  0.061667  NeuralLogicRec                 3   \n",
       "\n",
       "    nr_item_samples  precision    recall  \n",
       "0              4096        NaN  0.000000  \n",
       "1              4096        NaN  0.000000  \n",
       "2              4096        NaN  0.000000  \n",
       "3              4096        NaN  0.000000  \n",
       "4              4096        NaN  0.000000  \n",
       "5              4096        NaN  0.000000  \n",
       "6              4096        NaN  0.000000  \n",
       "7              4096   0.940640  0.166344  \n",
       "8              4096        NaN  0.000000  \n",
       "9              4096        NaN  0.000000  \n",
       "10             4096        NaN  0.000000  \n",
       "11             4096        NaN  0.000000  \n",
       "12             4096   0.919012  0.394483  \n",
       "13             4096   0.913462  0.360523  \n",
       "14             4096        NaN  0.000000  \n",
       "15             4096        NaN  0.000000  \n",
       "16             4096        NaN  0.000000  \n",
       "17             4096        NaN  0.000000  \n",
       "18             4096        NaN  0.000000  \n",
       "19             4096        NaN  0.000000  \n",
       "20             4096        NaN  0.000000  \n",
       "21             4096        NaN  0.000000  \n",
       "22             4096        NaN  0.000000  \n",
       "23             4096        NaN  0.000000  \n",
       "24             4096        NaN  0.000000  \n",
       "25             4096   0.906659  0.531894  \n",
       "26             4096        NaN  0.000000  \n",
       "27             4096        NaN  0.000000  \n",
       "28             4096        NaN  0.000000  \n",
       "29             4096   0.908678  0.449970  \n",
       "..              ...        ...       ...  \n",
       "68             4096   0.913619  0.591401  \n",
       "69             4096   0.918388  0.547955  \n",
       "70             4096   0.913823  0.548570  \n",
       "71             4096   0.918090  0.547848  \n",
       "72             4096   0.915649  0.539268  \n",
       "73             4096   0.922412  0.526786  \n",
       "74             4096   0.906270  0.575683  \n",
       "75             4096   0.926743  0.531698  \n",
       "76             4096   0.917149  0.557269  \n",
       "77             4096   0.914156  0.544806  \n",
       "78             4096   0.914632  0.553864  \n",
       "79             4096   0.919793  0.549394  \n",
       "80             4096   0.915871  0.560778  \n",
       "81             4096   0.916886  0.552758  \n",
       "82             4096        NaN  0.000000  \n",
       "83             4096        NaN  0.000000  \n",
       "84             4096   0.992278  0.003951  \n",
       "85             4096   0.921019  0.308441  \n",
       "86             4096   0.908315  0.262898  \n",
       "87             4096   0.957483  0.128667  \n",
       "88             4096   0.926901  0.187047  \n",
       "89             4096   0.928153  0.257070  \n",
       "90             4096        NaN  0.000000  \n",
       "91             4096        NaN  0.000000  \n",
       "92             4096        NaN  0.000000  \n",
       "93             4096   1.000000  0.000121  \n",
       "94             4096   0.936321  0.139600  \n",
       "95             4096   0.928513  0.257132  \n",
       "96             4096   0.926946  0.347996  \n",
       "97             4096   0.930746  0.098891  \n",
       "\n",
       "[98 rows x 20 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.DataFrame(evals[12:])\n",
    "pd.DataFrame(ae_evals)\n",
    "# pd.DataFrame(eval_per_epcoh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[5.01272e-01 4.91182e-01 4.81446e-01 ... 4.50313e-04 2.52783e-04\n",
      "  2.49922e-04]\n",
      " [4.87292e-01 4.79189e-01 2.37687e-01 ... 6.09666e-04 3.41058e-04\n",
      "  3.36915e-04]\n",
      " [4.60955e-01 2.31097e-01 2.54655e-02 ... 2.77162e-06 1.51992e-06\n",
      "  1.57952e-06]\n",
      " ...\n",
      " [3.96794e-01 3.86981e-01 3.72292e-01 ... 9.23872e-07 5.06639e-07\n",
      "  5.06639e-07]\n",
      " [4.72577e-01 2.23054e-01 5.88066e-02 ... 4.74155e-05 2.65241e-05\n",
      "  2.62856e-05]\n",
      " [4.44830e-01 4.30209e-01 1.88496e-01 ... 2.83122e-06 1.57161e-06\n",
      "  1.55245e-06]], shape=(10, 10381), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[4.24262e-01 1.67903e-01 8.53726e-03 ... 2.20835e-05 4.78417e-04\n",
      "  5.07504e-04]\n",
      " [9.17219e-01 1.47443e-01 3.45615e-01 ... 5.88363e-03 1.23903e-03\n",
      "  1.27688e-03]\n",
      " [9.90472e-01 9.98128e-01 4.85979e-02 ... 6.12944e-04 2.09212e-05\n",
      "  2.08020e-05]\n",
      " ...\n",
      " [9.70988e-01 3.42727e-06 9.78886e-01 ... 1.69873e-06 4.17233e-07\n",
      "  4.17233e-07]\n",
      " [9.99542e-01 9.87355e-01 9.23099e-01 ... 1.00055e-03 1.01954e-04\n",
      "  8.50558e-05]\n",
      " [6.88449e-01 4.94719e-06 9.57851e-01 ... 1.63913e-06 2.90643e-06\n",
      "  3.29926e-06]], shape=(10, 10381), dtype=float32)\n",
      "[ 49 302 338 502]\n"
     ]
    }
   ],
   "source": [
    "# print(np.flip(np.argsort(nlr.predict_single_user(3))))\n",
    "np.set_printoptions(precision=5)\n",
    "\n",
    "ds = utils.common.load_dataset(ml_small).shuffle(128).batch(10).take(1)\n",
    "for data in ds:\n",
    "    users = data['user_id']\n",
    "    rated = tf.cast(data['x'], tf.float32)\n",
    "    mask =  tf.cast(data['mask'], tf.float32)\n",
    "    pp = nlr.model(users, rated, mask)\n",
    "    print((pp['rec']))\n",
    "    print((pp['likes']))\n",
    "    # print(np.std(pp['rated']))\n",
    "    # print(np.argsort((np.std(pp['rec'], axis=0))))\n",
    "    top_1 = np.flip(np.argsort(pp['rec']))\n",
    "    print(np.unique(top_1[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_data = pd.DataFrame(ae_evals[-8:])\n",
    "eval_data = eval_data.sort_values(by='epochs_trained')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(eval_data['epochs_trained'], eval_data['diversity@5'])\n",
    "plt.plot(eval_data['epochs_trained'], eval_data['map@5'])\n",
    "plt.plot(eval_data['epochs_trained'], eval_data['map@1'])\n",
    "plt.show()\n",
    "plt.bar(eval_data.index, eval_data['map@1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
