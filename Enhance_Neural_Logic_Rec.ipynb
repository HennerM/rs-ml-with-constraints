{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.common\n",
    "import evaluation\n",
    "import importlib\n",
    "import numpy as np\n",
    "import time\n",
    "from models.ConstraintAutoRec import ConstraintAutoRec \n",
    "import models.NeuralLogicRec\n",
    "import tensorflow as tf\n",
    "importlib.reload(utils.common)\n",
    "importlib.reload(evaluation)\n",
    "importlib.reload(models.NeuralLogicRec)\n",
    "import itertools\n",
    "\n",
    "ml_small = utils.common.ml_small\n",
    "\n",
    "# ev = evaluation.Evaluation(ml_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 Loss at step 2: 0.1983, time: 2.934\n",
      "Epoch #1 Loss at step 2: 0.1907, time: 2.125\n",
      "Epoch #2 Loss at step 2: 0.1890, time: 1.964\n",
      "Epoch #3 Loss at step 2: 0.1864, time: 1.928\n",
      "Epoch #4 Loss at step 2: 0.1834, time: 1.843\n",
      "Epoch #5 Loss at step 2: 0.1823, time: 1.758\n",
      "Epoch #6 Loss at step 2: 0.1812, time: 1.769\n",
      "Epoch #7 Loss at step 2: 0.1789, time: 1.672\n",
      "Epoch #8 Loss at step 2: 0.1757, time: 1.660\n",
      "Epoch #9 Loss at step 2: 0.1725, time: 1.729\n",
      "Epoch #10 Loss at step 2: 0.1678, time: 1.701\n",
      "Epoch #11 Loss at step 2: 0.1620, time: 1.831\n",
      "Epoch #12 Loss at step 2: 0.1570, time: 1.791\n",
      "Epoch #13 Loss at step 2: 0.1516, time: 1.861\n",
      "Epoch #14 Loss at step 2: 0.1464, time: 1.794\n",
      "Epoch #15 Loss at step 2: 0.1415, time: 1.862\n",
      "Epoch #16 Loss at step 2: 0.1372, time: 1.789\n",
      "Epoch #17 Loss at step 2: 0.1336, time: 1.871\n",
      "Epoch #18 Loss at step 2: 0.1306, time: 1.809\n",
      "Epoch #19 Loss at step 2: 0.1286, time: 1.843\n",
      "Epoch #20 Loss at step 2: 0.1266, time: 1.818\n",
      "Epoch #21 Loss at step 2: 0.1250, time: 1.830\n",
      "Epoch #22 Loss at step 2: 0.1236, time: 1.768\n",
      "Epoch #23 Loss at step 2: 0.1224, time: 1.795\n",
      "Epoch #24 Loss at step 2: 0.1212, time: 1.731\n",
      "Epoch #25 Loss at step 2: 0.1205, time: 1.770\n",
      "Epoch #26 Loss at step 2: 0.1194, time: 1.686\n",
      "Epoch #27 Loss at step 2: 0.1183, time: 1.771\n",
      "Epoch #28 Loss at step 2: 0.1173, time: 1.695\n",
      "Epoch #29 Loss at step 2: 0.1162, time: 1.722\n",
      "Epoch #30 Loss at step 2: 0.1156, time: 1.825\n",
      "Epoch #31 Loss at step 2: 0.1147, time: 1.745\n",
      "Epoch #32 Loss at step 2: 0.1136, time: 1.780\n",
      "Epoch #33 Loss at step 2: 0.1127, time: 1.703\n",
      "Epoch #34 Loss at step 2: 0.1117, time: 1.778\n",
      "Epoch #35 Loss at step 2: 0.1111, time: 1.666\n",
      "Epoch #36 Loss at step 2: 0.1101, time: 1.734\n",
      "Epoch #37 Loss at step 2: 0.1107, time: 1.629\n",
      "Epoch #38 Loss at step 2: 0.1106, time: 1.662\n",
      "Epoch #39 Loss at step 2: 0.1318, time: 1.729\n",
      "Epoch #40 Loss at step 2: 0.1483, time: 1.799\n",
      "Epoch #41 Loss at step 2: 0.1373, time: 1.889\n",
      "Epoch #42 Loss at step 2: 0.1265, time: 1.992\n",
      "Epoch #43 Loss at step 2: 0.1011, time: 2.207\n",
      "Epoch #44 Loss at step 2: 0.1324, time: 2.307\n",
      "Epoch #45 Loss at step 2: 0.0686, time: 2.245\n",
      "Epoch #46 Loss at step 2: 0.0031, time: 2.157\n",
      "Epoch #47 Loss at step 2: -0.0620, time: 2.233\n",
      "Epoch #48 Loss at step 2: -0.1427, time: 2.220\n",
      "Epoch #49 Loss at step 2: -0.2341, time: 2.184\n",
      "Epoch #50 Loss at step 2: -0.4290, time: 2.455\n",
      "Epoch #51 Loss at step 2: -0.4941, time: 2.263\n",
      "Epoch #52 Loss at step 2: -0.5759, time: 2.262\n",
      "Epoch #53 Loss at step 2: -0.6390, time: 2.284\n",
      "Epoch #54 Loss at step 2: -0.6897, time: 2.517\n",
      "Epoch #55 Loss at step 2: -0.7617, time: 2.309\n",
      "Epoch #56 Loss at step 2: -0.8157, time: 2.313\n",
      "Epoch #57 Loss at step 2: -0.8784, time: 2.349\n",
      "Epoch #58 Loss at step 2: -0.9293, time: 2.631\n",
      "Epoch #59 Loss at step 2: -0.9782, time: 2.477\n",
      "Epoch #60 Loss at step 2: -1.0360, time: 2.406\n",
      "Epoch #61 Loss at step 2: -1.0641, time: 2.458\n",
      "Epoch #62 Loss at step 2: -1.1094, time: 2.453\n",
      "Epoch #63 Loss at step 2: -1.1328, time: 2.407\n",
      "Epoch #64 Loss at step 2: -1.1861, time: 2.553\n",
      "Epoch #65 Loss at step 2: -1.2350, time: 2.935\n",
      "Epoch #66 Loss at step 2: -1.2846, time: 2.495\n",
      "Epoch #67 Loss at step 2: -1.3361, time: 2.577\n",
      "Epoch #68 Loss at step 2: -1.3744, time: 2.441\n",
      "Epoch #69 Loss at step 2: -1.4254, time: 2.441\n",
      "Epoch #70 Loss at step 2: -1.4648, time: 2.434\n",
      "Epoch #71 Loss at step 2: -1.5044, time: 2.780\n",
      "Epoch #72 Loss at step 2: -1.5550, time: 2.215\n",
      "Epoch #73 Loss at step 2: -1.6000, time: 2.315\n",
      "Epoch #74 Loss at step 2: -1.6253, time: 2.311\n",
      "Epoch #75 Loss at step 2: -1.6698, time: 2.306\n",
      "Epoch #76 Loss at step 2: -1.7150, time: 2.304\n",
      "Epoch #77 Loss at step 2: -1.7407, time: 2.236\n",
      "Epoch #78 Loss at step 2: -1.7701, time: 2.315\n",
      "Epoch #79 Loss at step 2: -1.8174, time: 2.337\n",
      "Epoch #80 Loss at step 2: -1.8493, time: 2.248\n",
      "Epoch #81 Loss at step 2: -1.8891, time: 2.287\n",
      "Epoch #82 Loss at step 2: -1.9386, time: 2.223\n",
      "Epoch #83 Loss at step 2: -1.9865, time: 2.258\n",
      "Epoch #84 Loss at step 2: -2.0347, time: 2.246\n",
      "Epoch #85 Loss at step 2: -2.0749, time: 2.252\n",
      "Epoch #86 Loss at step 2: -2.1233, time: 2.179\n",
      "Epoch #87 Loss at step 2: -2.1729, time: 2.262\n",
      "Epoch #88 Loss at step 2: -2.2185, time: 2.266\n",
      "Epoch #89 Loss at step 2: -2.2614, time: 2.213\n",
      "Epoch #90 Loss at step 2: -2.3079, time: 2.236\n",
      "Epoch #91 Loss at step 2: -2.3555, time: 2.233\n",
      "Epoch #92 Loss at step 2: -2.3999, time: 2.216\n",
      "Epoch #93 Loss at step 2: -2.4483, time: 2.168\n",
      "Epoch #94 Loss at step 2: -2.4913, time: 2.244\n",
      "Epoch #95 Loss at step 2: -2.5386, time: 2.255\n",
      "Epoch #96 Loss at step 2: -2.5841, time: 2.264\n",
      "Epoch #97 Loss at step 2: -2.6291, time: 2.174\n",
      "Epoch #98 Loss at step 2: -2.6747, time: 2.292\n",
      "Epoch #99 Loss at step 2: -2.7214, time: 2.230\n",
      "Epoch #100 Loss at step 2: -2.7610, time: 2.262\n",
      "Epoch #101 Loss at step 2: -2.8038, time: 2.187\n",
      "Epoch #102 Loss at step 2: -2.8599, time: 2.259\n",
      "Epoch #103 Loss at step 2: -2.8942, time: 2.253\n",
      "Epoch #104 Loss at step 2: -2.9351, time: 2.293\n",
      "Epoch #105 Loss at step 2: -2.9817, time: 2.361\n",
      "Epoch #106 Loss at step 2: -3.0309, time: 2.439\n",
      "Epoch #107 Loss at step 2: -3.0600, time: 2.431\n",
      "Epoch #108 Loss at step 2: -3.1034, time: 2.418\n",
      "Epoch #109 Loss at step 2: -3.1413, time: 2.411\n",
      "Epoch #110 Loss at step 2: -3.1829, time: 2.368\n",
      "Epoch #111 Loss at step 2: -3.2219, time: 2.353\n",
      "Epoch #112 Loss at step 2: -3.2733, time: 2.315\n",
      "Epoch #113 Loss at step 2: -3.3093, time: 2.401\n",
      "Epoch #114 Loss at step 2: -3.3637, time: 2.236\n",
      "Epoch #115 Loss at step 2: -3.3924, time: 2.310\n",
      "Epoch #116 Loss at step 2: -3.4276, time: 2.284\n",
      "Epoch #117 Loss at step 2: -3.4749, time: 2.321\n",
      "Epoch #118 Loss at step 2: -3.5088, time: 2.291\n",
      "Epoch #119 Loss at step 2: -3.5500, time: 2.236\n",
      "Epoch #120 Loss at step 2: -3.5894, time: 2.300\n",
      "Epoch #121 Loss at step 2: -3.6236, time: 2.277\n",
      "Epoch #122 Loss at step 2: -3.6639, time: 2.312\n",
      "Epoch #123 Loss at step 2: -3.7026, time: 2.246\n",
      "Epoch #124 Loss at step 2: -3.7398, time: 2.302\n",
      "Epoch #125 Loss at step 2: -3.7794, time: 2.325\n",
      "Epoch #126 Loss at step 2: -3.8133, time: 2.304\n",
      "Epoch #127 Loss at step 2: -3.8449, time: 2.309\n",
      "Epoch #128 Loss at step 2: -3.8838, time: 2.241\n",
      "Epoch #129 Loss at step 2: -3.9267, time: 2.326\n",
      "Epoch #130 Loss at step 2: -3.9598, time: 2.312\n",
      "Epoch #131 Loss at step 2: -3.9918, time: 2.368\n",
      "Epoch #132 Loss at step 2: -4.0237, time: 2.226\n",
      "Epoch #133 Loss at step 2: -4.0676, time: 2.318\n",
      "Epoch #134 Loss at step 2: -4.1082, time: 2.309\n",
      "Epoch #135 Loss at step 2: -4.1466, time: 2.298\n",
      "Epoch #136 Loss at step 2: -4.1786, time: 2.223\n",
      "Epoch #137 Loss at step 2: -4.2227, time: 2.435\n",
      "Epoch #138 Loss at step 2: -4.2385, time: 2.383\n",
      "Epoch #139 Loss at step 2: -4.2819, time: 2.398\n",
      "Epoch #140 Loss at step 2: -4.3181, time: 2.305\n",
      "Epoch #141 Loss at step 2: -4.3534, time: 2.373\n",
      "Epoch #142 Loss at step 2: -4.3907, time: 2.303\n",
      "Epoch #143 Loss at step 2: -4.4194, time: 2.315\n",
      "Epoch #144 Loss at step 2: -4.4635, time: 2.308\n",
      "Epoch #145 Loss at step 2: -4.4940, time: 2.413\n",
      "Epoch #146 Loss at step 2: -4.5292, time: 2.336\n",
      "Epoch #147 Loss at step 2: -4.5564, time: 2.351\n",
      "Epoch #148 Loss at step 2: -4.5863, time: 2.325\n",
      "Epoch #149 Loss at step 2: -4.6290, time: 2.263\n",
      "Epoch #150 Loss at step 2: -4.6738, time: 2.333\n",
      "Epoch #151 Loss at step 2: -4.7056, time: 2.342\n",
      "Epoch #152 Loss at step 2: -4.7269, time: 2.342\n",
      "Epoch #153 Loss at step 2: -4.7718, time: 2.364\n",
      "Epoch #154 Loss at step 2: -4.8056, time: 2.261\n",
      "Epoch #155 Loss at step 2: -4.8454, time: 2.344\n",
      "Epoch #156 Loss at step 2: -4.8820, time: 2.340\n",
      "Epoch #157 Loss at step 2: -4.9054, time: 2.336\n",
      "Epoch #158 Loss at step 2: -4.9390, time: 2.340\n",
      "Epoch #159 Loss at step 2: -4.9716, time: 2.309\n",
      "Epoch #160 Loss at step 2: -4.9973, time: 2.368\n",
      "Epoch #161 Loss at step 2: -5.0379, time: 2.355\n",
      "Epoch #162 Loss at step 2: -5.0798, time: 2.363\n",
      "Epoch #163 Loss at step 2: -5.1008, time: 2.229\n",
      "Epoch #164 Loss at step 2: -5.1421, time: 2.292\n",
      "Epoch #165 Loss at step 2: -5.1698, time: 2.323\n",
      "Epoch #166 Loss at step 2: -5.2084, time: 2.307\n",
      "Epoch #167 Loss at step 2: -5.2352, time: 2.304\n",
      "Epoch #168 Loss at step 2: -5.2786, time: 2.376\n",
      "Epoch #169 Loss at step 2: -5.3129, time: 2.438\n",
      "Epoch #170 Loss at step 2: -5.3405, time: 2.428\n",
      "Epoch #171 Loss at step 2: -5.3816, time: 2.428\n",
      "Epoch #172 Loss at step 2: -5.4179, time: 2.368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #173 Loss at step 2: -5.4487, time: 2.290\n",
      "Epoch #174 Loss at step 2: -5.4723, time: 3.150\n",
      "Epoch #175 Loss at step 2: -5.5019, time: 3.508\n",
      "Epoch #176 Loss at step 2: -5.5319, time: 3.151\n",
      "Epoch #177 Loss at step 2: -5.5681, time: 2.962\n",
      "Epoch #178 Loss at step 2: -5.6093, time: 3.085\n",
      "Epoch #179 Loss at step 2: -5.6451, time: 2.784\n",
      "Epoch #180 Loss at step 2: -5.6546, time: 3.080\n",
      "Epoch #181 Loss at step 2: -5.6975, time: 2.928\n",
      "Epoch #182 Loss at step 2: -5.7357, time: 3.799\n",
      "Epoch #183 Loss at step 2: -5.7629, time: 3.044\n",
      "Epoch #184 Loss at step 2: -5.7949, time: 2.572\n",
      "Epoch #185 Loss at step 2: -5.8350, time: 2.645\n",
      "Epoch #186 Loss at step 2: -5.8552, time: 2.564\n",
      "Epoch #187 Loss at step 2: -5.8891, time: 2.561\n",
      "Epoch #188 Loss at step 2: -5.9096, time: 2.614\n",
      "Epoch #189 Loss at step 2: -5.9560, time: 2.634\n",
      "Epoch #190 Loss at step 2: -5.9965, time: 2.532\n",
      "Epoch #191 Loss at step 2: -6.0100, time: 2.573\n",
      "Epoch #192 Loss at step 2: -6.0595, time: 2.569\n",
      "Epoch #193 Loss at step 2: -6.0917, time: 2.507\n",
      "Epoch #194 Loss at step 2: -6.1063, time: 2.539\n",
      "Epoch #195 Loss at step 2: -6.1431, time: 2.293\n",
      "Epoch #196 Loss at step 2: -6.1655, time: 2.371\n",
      "Epoch #197 Loss at step 2: -6.2191, time: 2.411\n",
      "Epoch #198 Loss at step 2: -6.2379, time: 2.432\n",
      "Epoch #199 Loss at step 2: -6.2889, time: 2.369\n",
      "Epoch #200 Loss at step 2: -6.2915, time: 2.293\n",
      "Epoch #201 Loss at step 2: -6.3333, time: 2.324\n",
      "Epoch #202 Loss at step 2: -6.3740, time: 2.382\n",
      "Epoch #203 Loss at step 2: -6.4060, time: 2.378\n",
      "Epoch #204 Loss at step 2: -6.4312, time: 2.498\n",
      "Epoch #205 Loss at step 2: -6.4779, time: 2.403\n",
      "Epoch #206 Loss at step 2: -6.4848, time: 2.486\n",
      "Epoch #207 Loss at step 2: -6.5377, time: 2.487\n",
      "Epoch #208 Loss at step 2: -6.5709, time: 2.463\n",
      "Epoch #209 Loss at step 2: -6.6010, time: 2.702\n",
      "Epoch #210 Loss at step 2: -6.6200, time: 2.602\n",
      "Epoch #211 Loss at step 2: -6.6532, time: 2.750\n",
      "Epoch #212 Loss at step 2: -6.6774, time: 2.429\n",
      "Epoch #213 Loss at step 2: -6.7194, time: 2.406\n",
      "Epoch #214 Loss at step 2: -6.7404, time: 2.302\n",
      "Epoch #215 Loss at step 2: -6.7672, time: 2.394\n",
      "Epoch #216 Loss at step 2: -6.8256, time: 2.387\n",
      "Epoch #217 Loss at step 2: -6.8309, time: 2.417\n",
      "Epoch #218 Loss at step 2: -6.8858, time: 2.389\n",
      "Epoch #219 Loss at step 2: -6.9056, time: 2.326\n",
      "Epoch #220 Loss at step 2: -6.9384, time: 2.405\n",
      "Epoch #221 Loss at step 2: -6.9626, time: 2.414\n",
      "Epoch #222 Loss at step 2: -7.0105, time: 2.392\n",
      "Epoch #223 Loss at step 2: -7.0305, time: 2.411\n",
      "Epoch #224 Loss at step 2: -7.0479, time: 2.328\n",
      "Epoch #225 Loss at step 2: -7.0850, time: 2.383\n",
      "Epoch #226 Loss at step 2: -7.1283, time: 2.390\n",
      "Epoch #227 Loss at step 2: -7.1510, time: 2.418\n",
      "Epoch #228 Loss at step 2: -7.1730, time: 2.367\n",
      "Epoch #229 Loss at step 2: -7.2087, time: 2.369\n",
      "Epoch #230 Loss at step 2: -7.2421, time: 2.385\n",
      "Epoch #231 Loss at step 2: -7.2869, time: 2.365\n",
      "Epoch #232 Loss at step 2: -7.3022, time: 2.373\n",
      "Epoch #233 Loss at step 2: -7.3470, time: 2.444\n",
      "Epoch #234 Loss at step 2: -7.3797, time: 2.404\n",
      "Epoch #235 Loss at step 2: -7.4020, time: 2.446\n",
      "Epoch #236 Loss at step 2: -7.4120, time: 2.494\n",
      "Epoch #237 Loss at step 2: -7.4486, time: 2.474\n",
      "Epoch #238 Loss at step 2: -7.4965, time: 2.401\n",
      "Epoch #239 Loss at step 2: -7.5039, time: 2.310\n",
      "Epoch #240 Loss at step 2: -7.5248, time: 2.461\n",
      "Epoch #241 Loss at step 2: -7.5907, time: 2.499\n",
      "Epoch #242 Loss at step 2: -7.6286, time: 2.434\n",
      "Epoch #243 Loss at step 2: -7.6456, time: 2.436\n",
      "Epoch #244 Loss at step 2: -7.6713, time: 2.372\n",
      "Epoch #245 Loss at step 2: -7.6917, time: 2.487\n",
      "Epoch #246 Loss at step 2: -7.7312, time: 2.438\n",
      "Epoch #247 Loss at step 2: -7.7502, time: 2.400\n",
      "Epoch #248 Loss at step 2: -7.7997, time: 2.375\n",
      "Epoch #249 Loss at step 2: -7.8447, time: 2.318\n"
     ]
    }
   ],
   "source": [
    "nlr = models.NeuralLogicRec.NLR(ml_small['user'], ml_small['dimensions'], epochs=250, embedding_dim =16, batch_size=65_000)\n",
    "nlr.train(utils.common.load_dataset(ml_small), ml_small['train']['records'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch nr 1 predicted\n",
      "waiting for queue\n",
      "processing results\n",
      "Evaluating NeuralLogicRec |============================================================>| 100.0% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.42305061559507523,\n",
       " 'precision': 0.8186195826645265,\n",
       " 'recall': 0.4111245465538089,\n",
       " 'map@1': 0.0,\n",
       " 'map@5': 0.0,\n",
       " 'map@10': 0.0,\n",
       " 'diversity@5': 0.3847025733572199,\n",
       " 'diversity@10': 0.38171084228943014,\n",
       " 'epc@5': 0.9967813559149054,\n",
       " 'epc@10': 0.9966434971284364,\n",
       " 'epd@5': 0.38552351696625875,\n",
       " 'name': 'NeuralLogicRec',\n",
       " 'latent_dim': 128,\n",
       " 'epochs': 20,\n",
       " 'batch_size': 65000}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(np.flip(np.argsort(nlr.predict_single_user(3))))\n",
    "# print(np.sort(nlr.predict_single_user(1)))\n",
    "# nlr.model(np.array(1), np.array(1))\n",
    "ev.evaluate(nlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = tf.convert_to_tensor([x for x in itertools.permutations(range(600), 2)])\n",
    "u1 = u[:,0]\n",
    "u2 = u[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_u1 = tf.nn.embedding_lookup(nlr.model.user_embedding, u1)\n",
    "embed_u2 = tf.nn.embedding_lookup(nlr.model.user_embedding, u2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "359400"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cosine_sim(a, b):\n",
    "    cos_similarity = tf.keras.losses.cosine_similarity(a, b,axis=1)\n",
    "    return ( 1 + cos_similarity) / 2\n",
    "    \n",
    "len(cosine_sim(embed_u1, embed_u2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
